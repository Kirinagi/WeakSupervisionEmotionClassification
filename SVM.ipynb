{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8001ef74",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtarfile\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mspacy\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msnorkel\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "import spacy\n",
    "import re\n",
    "import snorkel\n",
    "import pandas as pd\n",
    "import rubrix as rb\n",
    "import numpy as np\n",
    "import json\n",
    "import sklearn\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from flashtext import KeywordProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "219e2d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scraped2 = pd.read_csv('D:\\Downloads\\Clean_data_Twitter_Emotion_Scraped_v5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7703e4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pradi\\AppData\\Local\\Temp\\ipykernel_37520\\3795501739.py:3: DtypeWarning: Columns (9,22,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data_scraped = pd.read_csv(\"D:\\Downloads\\Twitter_Data_Emotion_Scraped.csv\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Soal jln Jatibaru,polisi tdk bs GERTAK gubernu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sesama cewe lho (kayaknya), harusnya bisa lebi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kepingin gudeg mbarek Bu hj. Amad Foto dari go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jln Jatibaru,bagian dari wilayah Tn Abang.Peng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sharing pengalaman aja, kemarin jam 18.00 bata...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Mudah2an sudah terupload smua sebelum z mudik ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Orang pendukung khilafah memang harus di black...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>jangan sok akrab ye mention mention gue , mali...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Alhamdulillah Prof, setelah berbicara semalam ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Keren!! Kira-kira masih ada nggak yg bilang Pa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tweet\n",
       "0   Soal jln Jatibaru,polisi tdk bs GERTAK gubernu...\n",
       "1   Sesama cewe lho (kayaknya), harusnya bisa lebi...\n",
       "2   Kepingin gudeg mbarek Bu hj. Amad Foto dari go...\n",
       "3   Jln Jatibaru,bagian dari wilayah Tn Abang.Peng...\n",
       "4   Sharing pengalaman aja, kemarin jam 18.00 bata...\n",
       "..                                                ...\n",
       "95  Mudah2an sudah terupload smua sebelum z mudik ...\n",
       "96  Orang pendukung khilafah memang harus di black...\n",
       "97  jangan sok akrab ye mention mention gue , mali...\n",
       "98  Alhamdulillah Prof, setelah berbicara semalam ...\n",
       "99  Keren!! Kira-kira masih ada nggak yg bilang Pa...\n",
       "\n",
       "[100 rows x 1 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"D:\\Downloads\\Twitter_Emotion_Dataset_without_label.csv\")\n",
    "data_labelled = pd.read_csv(\"D:\\Downloads\\Twitter_Emotion_Dataset_with_label.csv\")\n",
    "data_scraped = pd.read_csv(\"D:\\Downloads\\Twitter_Data_Emotion_Scraped.csv\")\n",
    "dictionary = pd.read_csv(\"D:\\Downloads\\colloquial-indonesian-lexicon.csv\")\n",
    "data.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19253d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06a027b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Budayakan mencari dan membaca sebelum nanya ya. Udah dijelaskan berkali2 kok. Selama ga diatur/gaada ketentuan khusus, pakai aja'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_labelled['tweet'].iloc[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0aa5944c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>love</th>\n",
       "      <th>happy</th>\n",
       "      <th>sadness</th>\n",
       "      <th>anger</th>\n",
       "      <th>fear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anger</td>\n",
       "      <td>Soal jln Jatibaru,polisi tdk bs GERTAK gubernu...</td>\n",
       "      <td>0.968565</td>\n",
       "      <td>0.014053</td>\n",
       "      <td>0.008348</td>\n",
       "      <td>0.006490</td>\n",
       "      <td>0.002544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anger</td>\n",
       "      <td>Sesama cewe lho (kayaknya), harusnya bisa lebi...</td>\n",
       "      <td>0.611235</td>\n",
       "      <td>0.207921</td>\n",
       "      <td>0.090085</td>\n",
       "      <td>0.054547</td>\n",
       "      <td>0.036212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>happy</td>\n",
       "      <td>Kepingin gudeg mbarek Bu hj. Amad Foto dari go...</td>\n",
       "      <td>0.673907</td>\n",
       "      <td>0.120755</td>\n",
       "      <td>0.112393</td>\n",
       "      <td>0.051468</td>\n",
       "      <td>0.041477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>anger</td>\n",
       "      <td>Jln Jatibaru,bagian dari wilayah Tn Abang.Peng...</td>\n",
       "      <td>0.872383</td>\n",
       "      <td>0.052016</td>\n",
       "      <td>0.045204</td>\n",
       "      <td>0.019979</td>\n",
       "      <td>0.010417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>happy</td>\n",
       "      <td>Sharing pengalaman aja, kemarin jam 18.00 bata...</td>\n",
       "      <td>0.918236</td>\n",
       "      <td>0.049345</td>\n",
       "      <td>0.012681</td>\n",
       "      <td>0.010745</td>\n",
       "      <td>0.008993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>anger</td>\n",
       "      <td>Dari sekian banyak thread yang aku baca, threa...</td>\n",
       "      <td>0.475593</td>\n",
       "      <td>0.312061</td>\n",
       "      <td>0.181850</td>\n",
       "      <td>0.018317</td>\n",
       "      <td>0.012179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>happy</td>\n",
       "      <td>Sharing sama temen tuh emg guna bgt. Disaat lu...</td>\n",
       "      <td>0.440551</td>\n",
       "      <td>0.244054</td>\n",
       "      <td>0.182177</td>\n",
       "      <td>0.078657</td>\n",
       "      <td>0.054560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Orang lain kalau pake ponco itu buat jas hujan...</td>\n",
       "      <td>0.672150</td>\n",
       "      <td>0.148811</td>\n",
       "      <td>0.092678</td>\n",
       "      <td>0.071228</td>\n",
       "      <td>0.015133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>anger</td>\n",
       "      <td>Contoh mereka yg gemar menyudutkan, teriak pal...</td>\n",
       "      <td>0.858897</td>\n",
       "      <td>0.075341</td>\n",
       "      <td>0.028512</td>\n",
       "      <td>0.020424</td>\n",
       "      <td>0.016826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Pulang udah H-4 lebaran dilema sekali. Seperti...</td>\n",
       "      <td>0.461462</td>\n",
       "      <td>0.231217</td>\n",
       "      <td>0.122930</td>\n",
       "      <td>0.107638</td>\n",
       "      <td>0.076752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>anger</td>\n",
       "      <td>Betul Min rakyat Indonesia sekarang harapan ny...</td>\n",
       "      <td>0.421219</td>\n",
       "      <td>0.280273</td>\n",
       "      <td>0.202222</td>\n",
       "      <td>0.080475</td>\n",
       "      <td>0.015811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>anger</td>\n",
       "      <td>Kalaupun fansite ngejual hasil jepretan mereka...</td>\n",
       "      <td>0.344793</td>\n",
       "      <td>0.322116</td>\n",
       "      <td>0.181661</td>\n",
       "      <td>0.079476</td>\n",
       "      <td>0.071954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>happy</td>\n",
       "      <td>sangat bersyukur bisa mendoakan kakeknya, Bung...</td>\n",
       "      <td>0.824167</td>\n",
       "      <td>0.087022</td>\n",
       "      <td>0.056032</td>\n",
       "      <td>0.028119</td>\n",
       "      <td>0.004659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Sulit menerima kenyataan memang bahwa ada seba...</td>\n",
       "      <td>0.345635</td>\n",
       "      <td>0.274249</td>\n",
       "      <td>0.174274</td>\n",
       "      <td>0.136113</td>\n",
       "      <td>0.069728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>anger</td>\n",
       "      <td>samanye udeh maling semua gak si jancok gak si...</td>\n",
       "      <td>0.575221</td>\n",
       "      <td>0.369996</td>\n",
       "      <td>0.023698</td>\n",
       "      <td>0.017082</td>\n",
       "      <td>0.014003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>anger</td>\n",
       "      <td>Ku juga pengen ngamuk bacain komen2 netizen yg...</td>\n",
       "      <td>0.433143</td>\n",
       "      <td>0.323045</td>\n",
       "      <td>0.228793</td>\n",
       "      <td>0.010513</td>\n",
       "      <td>0.004506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>love</td>\n",
       "      <td>Setiap kesempatan yg pernah hadir tuk dapat me...</td>\n",
       "      <td>0.611309</td>\n",
       "      <td>0.194637</td>\n",
       "      <td>0.138605</td>\n",
       "      <td>0.035006</td>\n",
       "      <td>0.020443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>anger</td>\n",
       "      <td>Sedap kali kau ya bobok dikasur ku. Aku tidur ...</td>\n",
       "      <td>0.399549</td>\n",
       "      <td>0.211316</td>\n",
       "      <td>0.207526</td>\n",
       "      <td>0.141130</td>\n",
       "      <td>0.040479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>happy</td>\n",
       "      <td>H-9. Mau ke bank rame bgt ampe antrian tempat ...</td>\n",
       "      <td>0.409420</td>\n",
       "      <td>0.384678</td>\n",
       "      <td>0.109521</td>\n",
       "      <td>0.078088</td>\n",
       "      <td>0.018294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>anger</td>\n",
       "      <td>Benar2 hancur negara ini dikelola amatiran, tp...</td>\n",
       "      <td>0.963118</td>\n",
       "      <td>0.019981</td>\n",
       "      <td>0.008923</td>\n",
       "      <td>0.004187</td>\n",
       "      <td>0.003792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>anger</td>\n",
       "      <td>Budayakan mencari dan membaca sebelum nanya ya...</td>\n",
       "      <td>0.444329</td>\n",
       "      <td>0.359824</td>\n",
       "      <td>0.097076</td>\n",
       "      <td>0.065277</td>\n",
       "      <td>0.033495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Separuh hati ini iri jika melihat seorang anak...</td>\n",
       "      <td>0.537034</td>\n",
       "      <td>0.310352</td>\n",
       "      <td>0.098936</td>\n",
       "      <td>0.044391</td>\n",
       "      <td>0.009287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>happy</td>\n",
       "      <td>Ketika aku tersenyum bukan berarti hidupku sem...</td>\n",
       "      <td>0.593127</td>\n",
       "      <td>0.355605</td>\n",
       "      <td>0.030175</td>\n",
       "      <td>0.015835</td>\n",
       "      <td>0.005259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>sadness</td>\n",
       "      <td>dari mau tdr, tidur, sampe bgn tdr kok yaa kay...</td>\n",
       "      <td>0.827562</td>\n",
       "      <td>0.123559</td>\n",
       "      <td>0.022512</td>\n",
       "      <td>0.017564</td>\n",
       "      <td>0.008803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>love</td>\n",
       "      <td>kan kupeluk engkau erat2 hingga tak ada seoran...</td>\n",
       "      <td>0.587928</td>\n",
       "      <td>0.257988</td>\n",
       "      <td>0.084371</td>\n",
       "      <td>0.037011</td>\n",
       "      <td>0.032702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>anger</td>\n",
       "      <td>Kalo mau ganti presiden itu harus jelas siapa ...</td>\n",
       "      <td>0.944610</td>\n",
       "      <td>0.039594</td>\n",
       "      <td>0.006362</td>\n",
       "      <td>0.006214</td>\n",
       "      <td>0.003220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>happy</td>\n",
       "      <td>Sukses n keren banget dgn no.1 kualitas,bhn kr...</td>\n",
       "      <td>0.822646</td>\n",
       "      <td>0.132860</td>\n",
       "      <td>0.032260</td>\n",
       "      <td>0.008871</td>\n",
       "      <td>0.003363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>anger</td>\n",
       "      <td>Udah mau sarjana 2 kali, mbokya mulut nya jang...</td>\n",
       "      <td>0.624594</td>\n",
       "      <td>0.143481</td>\n",
       "      <td>0.098001</td>\n",
       "      <td>0.072013</td>\n",
       "      <td>0.061911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>anger</td>\n",
       "      <td>Gimana orang ga nilai dr jilbab/syari/nggak ny...</td>\n",
       "      <td>0.398171</td>\n",
       "      <td>0.280276</td>\n",
       "      <td>0.128716</td>\n",
       "      <td>0.108182</td>\n",
       "      <td>0.084655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>fear</td>\n",
       "      <td>Hari ini jadwal presentasi proker di LPPM karn...</td>\n",
       "      <td>0.900938</td>\n",
       "      <td>0.049128</td>\n",
       "      <td>0.028933</td>\n",
       "      <td>0.016586</td>\n",
       "      <td>0.004416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>anger</td>\n",
       "      <td>Foto saya di instagram masih ada cuma lupa pas...</td>\n",
       "      <td>0.536650</td>\n",
       "      <td>0.163816</td>\n",
       "      <td>0.145591</td>\n",
       "      <td>0.107356</td>\n",
       "      <td>0.046587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>fear</td>\n",
       "      <td>Drama kmarin sore : seharian puasa trus pas lg...</td>\n",
       "      <td>0.680161</td>\n",
       "      <td>0.200128</td>\n",
       "      <td>0.073986</td>\n",
       "      <td>0.026947</td>\n",
       "      <td>0.018778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Sungguh mencintaimu berat sekali. Berpisah den...</td>\n",
       "      <td>0.636124</td>\n",
       "      <td>0.137054</td>\n",
       "      <td>0.134512</td>\n",
       "      <td>0.058515</td>\n",
       "      <td>0.033794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>anger</td>\n",
       "      <td>4. pas uda dikirim kembali, si kurir telp ngel...</td>\n",
       "      <td>0.558021</td>\n",
       "      <td>0.269248</td>\n",
       "      <td>0.109409</td>\n",
       "      <td>0.040582</td>\n",
       "      <td>0.022741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>happy</td>\n",
       "      <td>Pada akhirnya kita akan melewati semua ini, se...</td>\n",
       "      <td>0.635503</td>\n",
       "      <td>0.297695</td>\n",
       "      <td>0.043941</td>\n",
       "      <td>0.015561</td>\n",
       "      <td>0.007301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>love</td>\n",
       "      <td>Kenapa pasangan seringkali diibaratkan sebagai...</td>\n",
       "      <td>0.676091</td>\n",
       "      <td>0.191670</td>\n",
       "      <td>0.105041</td>\n",
       "      <td>0.017680</td>\n",
       "      <td>0.009518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>fear</td>\n",
       "      <td>Ya tau kalo rasa sakit hati memamg sulit, tapi...</td>\n",
       "      <td>0.510861</td>\n",
       "      <td>0.466222</td>\n",
       "      <td>0.008531</td>\n",
       "      <td>0.008463</td>\n",
       "      <td>0.005923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>happy</td>\n",
       "      <td>Ponakan kls 4 SD: lg demen nonton video tutori...</td>\n",
       "      <td>0.426052</td>\n",
       "      <td>0.327495</td>\n",
       "      <td>0.131765</td>\n",
       "      <td>0.063916</td>\n",
       "      <td>0.050772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>happy</td>\n",
       "      <td>Bberapa tahun kemudian kesejahteraan keluarga ...</td>\n",
       "      <td>0.719570</td>\n",
       "      <td>0.155191</td>\n",
       "      <td>0.107954</td>\n",
       "      <td>0.009250</td>\n",
       "      <td>0.008035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Bukan tidak mendukung atau mendoakan, cuma men...</td>\n",
       "      <td>0.577487</td>\n",
       "      <td>0.228851</td>\n",
       "      <td>0.101716</td>\n",
       "      <td>0.066678</td>\n",
       "      <td>0.025269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>happy</td>\n",
       "      <td>JNE/JNT selama saya pake entah kirim paket/bel...</td>\n",
       "      <td>0.910053</td>\n",
       "      <td>0.045401</td>\n",
       "      <td>0.024287</td>\n",
       "      <td>0.012952</td>\n",
       "      <td>0.007307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>anger</td>\n",
       "      <td>Gemes bgt sama orang yg masih menganggap wajar...</td>\n",
       "      <td>0.694787</td>\n",
       "      <td>0.150803</td>\n",
       "      <td>0.111731</td>\n",
       "      <td>0.022830</td>\n",
       "      <td>0.019849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>fear</td>\n",
       "      <td>Apa hukumnya bermain musik stad? jelas HAROOOO...</td>\n",
       "      <td>0.599733</td>\n",
       "      <td>0.311133</td>\n",
       "      <td>0.076559</td>\n",
       "      <td>0.007230</td>\n",
       "      <td>0.005345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>anger</td>\n",
       "      <td>Apa Hebatnya Mardani? Menurut sy oeang ini Cum...</td>\n",
       "      <td>0.733980</td>\n",
       "      <td>0.196473</td>\n",
       "      <td>0.031626</td>\n",
       "      <td>0.020047</td>\n",
       "      <td>0.017873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Oh iya, ibu nya ngelahirin anak ke 3 yaitu cew...</td>\n",
       "      <td>0.858086</td>\n",
       "      <td>0.085067</td>\n",
       "      <td>0.027018</td>\n",
       "      <td>0.018838</td>\n",
       "      <td>0.010991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>happy</td>\n",
       "      <td>Z suka twitter soalny perempuan ji semua teman...</td>\n",
       "      <td>0.393002</td>\n",
       "      <td>0.228234</td>\n",
       "      <td>0.149023</td>\n",
       "      <td>0.147103</td>\n",
       "      <td>0.082638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>anger</td>\n",
       "      <td>Nggak ngerti lagi sama ciwi-ciwi yang nyinyiri...</td>\n",
       "      <td>0.850509</td>\n",
       "      <td>0.051907</td>\n",
       "      <td>0.050621</td>\n",
       "      <td>0.024154</td>\n",
       "      <td>0.022809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>anger</td>\n",
       "      <td>hari ini libur, rencananya mau nonton Jurassic...</td>\n",
       "      <td>0.291740</td>\n",
       "      <td>0.289814</td>\n",
       "      <td>0.190273</td>\n",
       "      <td>0.121752</td>\n",
       "      <td>0.106422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>anger</td>\n",
       "      <td>Nah ini, yg nunggu anak anteng dengan kesabara...</td>\n",
       "      <td>0.415981</td>\n",
       "      <td>0.333688</td>\n",
       "      <td>0.177661</td>\n",
       "      <td>0.042106</td>\n",
       "      <td>0.030563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>love</td>\n",
       "      <td>Ku rindu bisik mu di telingaku Seraya kau berk...</td>\n",
       "      <td>0.886026</td>\n",
       "      <td>0.071207</td>\n",
       "      <td>0.026480</td>\n",
       "      <td>0.013284</td>\n",
       "      <td>0.003003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                              tweet      love  \\\n",
       "0     anger  Soal jln Jatibaru,polisi tdk bs GERTAK gubernu...  0.968565   \n",
       "1     anger  Sesama cewe lho (kayaknya), harusnya bisa lebi...  0.611235   \n",
       "2     happy  Kepingin gudeg mbarek Bu hj. Amad Foto dari go...  0.673907   \n",
       "3     anger  Jln Jatibaru,bagian dari wilayah Tn Abang.Peng...  0.872383   \n",
       "4     happy  Sharing pengalaman aja, kemarin jam 18.00 bata...  0.918236   \n",
       "5     anger  Dari sekian banyak thread yang aku baca, threa...  0.475593   \n",
       "6     happy  Sharing sama temen tuh emg guna bgt. Disaat lu...  0.440551   \n",
       "7   sadness  Orang lain kalau pake ponco itu buat jas hujan...  0.672150   \n",
       "8     anger  Contoh mereka yg gemar menyudutkan, teriak pal...  0.858897   \n",
       "9   sadness  Pulang udah H-4 lebaran dilema sekali. Seperti...  0.461462   \n",
       "10    anger  Betul Min rakyat Indonesia sekarang harapan ny...  0.421219   \n",
       "11    anger  Kalaupun fansite ngejual hasil jepretan mereka...  0.344793   \n",
       "12    happy  sangat bersyukur bisa mendoakan kakeknya, Bung...  0.824167   \n",
       "13  sadness  Sulit menerima kenyataan memang bahwa ada seba...  0.345635   \n",
       "14    anger  samanye udeh maling semua gak si jancok gak si...  0.575221   \n",
       "15    anger  Ku juga pengen ngamuk bacain komen2 netizen yg...  0.433143   \n",
       "16     love  Setiap kesempatan yg pernah hadir tuk dapat me...  0.611309   \n",
       "17    anger  Sedap kali kau ya bobok dikasur ku. Aku tidur ...  0.399549   \n",
       "18    happy  H-9. Mau ke bank rame bgt ampe antrian tempat ...  0.409420   \n",
       "19    anger  Benar2 hancur negara ini dikelola amatiran, tp...  0.963118   \n",
       "20    anger  Budayakan mencari dan membaca sebelum nanya ya...  0.444329   \n",
       "21  sadness  Separuh hati ini iri jika melihat seorang anak...  0.537034   \n",
       "22    happy  Ketika aku tersenyum bukan berarti hidupku sem...  0.593127   \n",
       "23  sadness  dari mau tdr, tidur, sampe bgn tdr kok yaa kay...  0.827562   \n",
       "24     love  kan kupeluk engkau erat2 hingga tak ada seoran...  0.587928   \n",
       "25    anger  Kalo mau ganti presiden itu harus jelas siapa ...  0.944610   \n",
       "26    happy  Sukses n keren banget dgn no.1 kualitas,bhn kr...  0.822646   \n",
       "27    anger  Udah mau sarjana 2 kali, mbokya mulut nya jang...  0.624594   \n",
       "28    anger  Gimana orang ga nilai dr jilbab/syari/nggak ny...  0.398171   \n",
       "29     fear  Hari ini jadwal presentasi proker di LPPM karn...  0.900938   \n",
       "30    anger  Foto saya di instagram masih ada cuma lupa pas...  0.536650   \n",
       "31     fear  Drama kmarin sore : seharian puasa trus pas lg...  0.680161   \n",
       "32  sadness  Sungguh mencintaimu berat sekali. Berpisah den...  0.636124   \n",
       "33    anger  4. pas uda dikirim kembali, si kurir telp ngel...  0.558021   \n",
       "34    happy  Pada akhirnya kita akan melewati semua ini, se...  0.635503   \n",
       "35     love  Kenapa pasangan seringkali diibaratkan sebagai...  0.676091   \n",
       "36     fear  Ya tau kalo rasa sakit hati memamg sulit, tapi...  0.510861   \n",
       "37    happy  Ponakan kls 4 SD: lg demen nonton video tutori...  0.426052   \n",
       "38    happy  Bberapa tahun kemudian kesejahteraan keluarga ...  0.719570   \n",
       "39  sadness  Bukan tidak mendukung atau mendoakan, cuma men...  0.577487   \n",
       "40    happy  JNE/JNT selama saya pake entah kirim paket/bel...  0.910053   \n",
       "41    anger  Gemes bgt sama orang yg masih menganggap wajar...  0.694787   \n",
       "42     fear  Apa hukumnya bermain musik stad? jelas HAROOOO...  0.599733   \n",
       "43    anger  Apa Hebatnya Mardani? Menurut sy oeang ini Cum...  0.733980   \n",
       "44  sadness  Oh iya, ibu nya ngelahirin anak ke 3 yaitu cew...  0.858086   \n",
       "45    happy  Z suka twitter soalny perempuan ji semua teman...  0.393002   \n",
       "46    anger  Nggak ngerti lagi sama ciwi-ciwi yang nyinyiri...  0.850509   \n",
       "47    anger  hari ini libur, rencananya mau nonton Jurassic...  0.291740   \n",
       "48    anger  Nah ini, yg nunggu anak anteng dengan kesabara...  0.415981   \n",
       "49     love  Ku rindu bisik mu di telingaku Seraya kau berk...  0.886026   \n",
       "\n",
       "       happy   sadness     anger      fear  \n",
       "0   0.014053  0.008348  0.006490  0.002544  \n",
       "1   0.207921  0.090085  0.054547  0.036212  \n",
       "2   0.120755  0.112393  0.051468  0.041477  \n",
       "3   0.052016  0.045204  0.019979  0.010417  \n",
       "4   0.049345  0.012681  0.010745  0.008993  \n",
       "5   0.312061  0.181850  0.018317  0.012179  \n",
       "6   0.244054  0.182177  0.078657  0.054560  \n",
       "7   0.148811  0.092678  0.071228  0.015133  \n",
       "8   0.075341  0.028512  0.020424  0.016826  \n",
       "9   0.231217  0.122930  0.107638  0.076752  \n",
       "10  0.280273  0.202222  0.080475  0.015811  \n",
       "11  0.322116  0.181661  0.079476  0.071954  \n",
       "12  0.087022  0.056032  0.028119  0.004659  \n",
       "13  0.274249  0.174274  0.136113  0.069728  \n",
       "14  0.369996  0.023698  0.017082  0.014003  \n",
       "15  0.323045  0.228793  0.010513  0.004506  \n",
       "16  0.194637  0.138605  0.035006  0.020443  \n",
       "17  0.211316  0.207526  0.141130  0.040479  \n",
       "18  0.384678  0.109521  0.078088  0.018294  \n",
       "19  0.019981  0.008923  0.004187  0.003792  \n",
       "20  0.359824  0.097076  0.065277  0.033495  \n",
       "21  0.310352  0.098936  0.044391  0.009287  \n",
       "22  0.355605  0.030175  0.015835  0.005259  \n",
       "23  0.123559  0.022512  0.017564  0.008803  \n",
       "24  0.257988  0.084371  0.037011  0.032702  \n",
       "25  0.039594  0.006362  0.006214  0.003220  \n",
       "26  0.132860  0.032260  0.008871  0.003363  \n",
       "27  0.143481  0.098001  0.072013  0.061911  \n",
       "28  0.280276  0.128716  0.108182  0.084655  \n",
       "29  0.049128  0.028933  0.016586  0.004416  \n",
       "30  0.163816  0.145591  0.107356  0.046587  \n",
       "31  0.200128  0.073986  0.026947  0.018778  \n",
       "32  0.137054  0.134512  0.058515  0.033794  \n",
       "33  0.269248  0.109409  0.040582  0.022741  \n",
       "34  0.297695  0.043941  0.015561  0.007301  \n",
       "35  0.191670  0.105041  0.017680  0.009518  \n",
       "36  0.466222  0.008531  0.008463  0.005923  \n",
       "37  0.327495  0.131765  0.063916  0.050772  \n",
       "38  0.155191  0.107954  0.009250  0.008035  \n",
       "39  0.228851  0.101716  0.066678  0.025269  \n",
       "40  0.045401  0.024287  0.012952  0.007307  \n",
       "41  0.150803  0.111731  0.022830  0.019849  \n",
       "42  0.311133  0.076559  0.007230  0.005345  \n",
       "43  0.196473  0.031626  0.020047  0.017873  \n",
       "44  0.085067  0.027018  0.018838  0.010991  \n",
       "45  0.228234  0.149023  0.147103  0.082638  \n",
       "46  0.051907  0.050621  0.024154  0.022809  \n",
       "47  0.289814  0.190273  0.121752  0.106422  \n",
       "48  0.333688  0.177661  0.042106  0.030563  \n",
       "49  0.071207  0.026480  0.013284  0.003003  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_saputri.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9da4dd84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_scraped = data_scraped.drop(columns=['id', 'conversation_id','created_at','date','time','timezone','user_id','username','name','place','language','mentions','urls','photos','replies_count','retweets_count','likes_count','hashtags','cashtags','link','retweet','quote_url','video','thumbnail','near','geo','source','user_rt_id','user_rt','retweet_id','reply_to','retweet_date','translate','trans_src','trans_dest'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5c5d709",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = dictionary.drop(columns=['In-dictionary','context','category1','category2','category3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e00c3d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet    20496\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_scraped.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0af8d4a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "slang     15006\n",
       "formal    15006\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4321c419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                         cnu styli fpj\n",
       "1               malas nya nak gi stdavid walaupun dekat\n",
       "2     please refrain tapi buy if youre very sensitiv...\n",
       "3               mampu gelak jeeeee sorryyyy hahahhahaha\n",
       "4                                                      \n",
       "                            ...                        \n",
       "95                                                     \n",
       "96    i cannot give you the formula for success but ...\n",
       "97    menteri didik tinggi yb dato seri mohamed khal...\n",
       "98    the game after lu left i became scourge lu fki...\n",
       "99                          finally last day of january\n",
       "Name: tweet, Length: 100, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_scraped.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec90ce55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        ‰∫∫Áîü„Åå„Å©„Çì„Å™„Å´Ëæõ„Åè„Å¶„ÇÇÂâçÂêë„Åç„Å´ËÄÉ„Åà„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ  üíõ  üî•  Ÿ±€íŸá€íÿ±ÿ®Ÿé Ÿ±€íŸáŸÄÿ±ÿ®Ÿé...\n",
      "1              malas nya nak gi st.david walaupun dekat. üòê\n",
      "2        ‚Äî please refrain to buy if you‚Äôre very sensiti...\n",
      "3        @hrthhrth Mampu gelak jeeeee sorryyyy hahahhahaha\n",
      "4                                     @personatheorymy ü¶éü¶éü¶é\n",
      "                               ...                        \n",
      "20491                                 di draftnya nih wkwk\n",
      "20492    @kim_medap besok2 gue nitip lu kali med.. tang...\n",
      "20493    Kayanya seru kalo 2024 perang partai 1. Pdip, ...\n",
      "20494                     @liberouno_ Bremsek emg muntun ü•≤\n",
      "20495               ADA APAANSI KOK JD PADA SERING MUNCUL?\n",
      "Name: tweet, Length: 20496, dtype: object\n"
     ]
    }
   ],
   "source": [
    "stop_factory = StopWordRemoverFactory()\n",
    "stopword = stop_factory.create_stop_word_remover()\n",
    "stop = stopword.remove\n",
    "\n",
    "data_scraped = data_scraped['tweet']\n",
    "data_scraped = data_scraped.apply(stop)\n",
    "print(data_scraped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7680bca3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc1fa8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pradi\\AppData\\Local\\Temp\\ipykernel_15628\\5118527.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data_scraped = data_scraped.str.replace(r'@\\w+','')\n",
      "C:\\Users\\pradi\\AppData\\Local\\Temp\\ipykernel_15628\\5118527.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data_scraped = data_scraped.str.replace(r'https?://\\S+','')\n",
      "C:\\Users\\pradi\\AppData\\Local\\Temp\\ipykernel_15628\\5118527.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data_scraped = data_scraped.str.replace(r'\\b0\\d{2,3}[-.]?\\d{3,4}[-.]?\\d{3,4}\\b','')\n",
      "C:\\Users\\pradi\\AppData\\Local\\Temp\\ipykernel_15628\\5118527.py:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data_scraped = data_scraped.str.replace('[^a-zA-Z\\s]+','')\n"
     ]
    }
   ],
   "source": [
    "data_scraped = data_scraped.str.replace(r'@\\w+','')\n",
    "data_scraped = data_scraped.str.replace(r'https?://\\S+','')\n",
    "data_scraped = data_scraped.str.replace(r'\\b0\\d{2,3}[-.]?\\d{3,4}[-.]?\\d{3,4}\\b','')\n",
    "data_scraped = data_scraped.str.replace('[^a-zA-Z\\s]+','')\n",
    "data_scraped = data_scraped.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d80ae4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.columns = ['slang','formal']\n",
    "change_dict = dict(zip(dictionary.slang,dictionary.formal))\n",
    "keyword_processor = KeywordProcessor(case_sensitive=True)\n",
    "for k, v in change_dict.items():\n",
    "    keyword_processor.add_keyword(k,v)\n",
    "\n",
    "data_scraped = pd.DataFrame(list(map(keyword_processor.replace_keywords, \n",
    "                                     data_scraped)))\n",
    "data_scraped.columns = ['tweet']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "207d9fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 0                                              tweet\n",
      "0               0      Sekarang udah ada union tapiiiiiiiii.........\n",
      "1               1  2 people followed me and one person unfollowed...\n",
      "2               2                                        Sore yopp ?\n",
      "3               3                               Laper yaa mas? Wkwwk\n",
      "4               4                                      Oooh can wait\n",
      "...           ...                                                ...\n",
      "81128       81128  Emang udah paling bener gausah cerita apa? sm ...\n",
      "81129       81129                                   Merosotin cuy ??\n",
      "81130       81130          Lahh ternyata ada 1 fans nya yang komen ?\n",
      "81131       81131  2023-04-10 21:18:49 Employee commercial would ...\n",
      "81132       81132                 Gatau ko aku taunya Kontol Kejepit\n",
      "\n",
      "[81133 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data_scraped2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22772e7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4401"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_labelled['label'].count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16c03030",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labelled['label'] = data_labelled['label'].replace(['happy', 'anger', 'sadness', 'fear', 'love'], [0, 1, 2, 3, 4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0069709d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>timezone</th>\n",
       "      <th>user_id</th>\n",
       "      <th>username</th>\n",
       "      <th>name</th>\n",
       "      <th>place</th>\n",
       "      <th>...</th>\n",
       "      <th>geo</th>\n",
       "      <th>source</th>\n",
       "      <th>user_rt_id</th>\n",
       "      <th>user_rt</th>\n",
       "      <th>retweet_id</th>\n",
       "      <th>reply_to</th>\n",
       "      <th>retweet_date</th>\n",
       "      <th>translate</th>\n",
       "      <th>trans_src</th>\n",
       "      <th>trans_dest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1620273021507309568</td>\n",
       "      <td>1620273021507309568</td>\n",
       "      <td>2023-01-31 11:09:26 SE Asia Standard Time</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>11:09:26</td>\n",
       "      <td>700</td>\n",
       "      <td>600050060</td>\n",
       "      <td>juescha86</td>\n",
       "      <td>ÿ≠ÿµŸá ÿ™ÿßŸÖÿ± ‚ó•‚Ä¢ÔªúŸïŸàŸêÿ®ŸàŸÜ‚Ä¢ ‚ÄèÔ∫ßŸïŸìÿµŸçŸÖŸç‚Ä¢ÔªúŸïŸàŸêÿØŸì</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7893,113.9213,500km</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1620273019678785537</td>\n",
       "      <td>1620273019678785537</td>\n",
       "      <td>2023-01-31 11:09:25 SE Asia Standard Time</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>11:09:25</td>\n",
       "      <td>700</td>\n",
       "      <td>1399762345</td>\n",
       "      <td>yazlijufri</td>\n",
       "      <td>Jufri Jup</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7893,113.9213,500km</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1620273019305492480</td>\n",
       "      <td>1620273014205194242</td>\n",
       "      <td>2023-01-31 11:09:25 SE Asia Standard Time</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>11:09:25</td>\n",
       "      <td>700</td>\n",
       "      <td>803928343049486336</td>\n",
       "      <td>naaerino</td>\n",
       "      <td>naaaa | semi IA ‚úø</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7893,113.9213,500km</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1620273018898624514</td>\n",
       "      <td>1620258168898199554</td>\n",
       "      <td>2023-01-31 11:09:25 SE Asia Standard Time</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>11:09:25</td>\n",
       "      <td>700</td>\n",
       "      <td>3093417654</td>\n",
       "      <td>zapyy0</td>\n",
       "      <td>zap</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7893,113.9213,500km</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'screen_name': 'hrthhrth', 'name': 'H', 'id'...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1620273018038808583</td>\n",
       "      <td>1620270912854724610</td>\n",
       "      <td>2023-01-31 11:09:25 SE Asia Standard Time</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>11:09:25</td>\n",
       "      <td>700</td>\n",
       "      <td>71463675</td>\n",
       "      <td>junchgaming</td>\n",
       "      <td>Junch is Taipei Game Show</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7893,113.9213,500km</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'screen_name': 'personatheorymy', 'name': 'P...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1620273017627766786</td>\n",
       "      <td>1620242295319842819</td>\n",
       "      <td>2023-01-31 11:09:25 SE Asia Standard Time</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>11:09:25</td>\n",
       "      <td>700</td>\n",
       "      <td>1166573925033218049</td>\n",
       "      <td>tujuhderajat0_0</td>\n",
       "      <td>Lindemann</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7893,113.9213,500km</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'screen_name': 'KomediRetjeh', 'name': 'Kome...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1620273016084250624</td>\n",
       "      <td>1620273016084250624</td>\n",
       "      <td>2023-01-31 11:09:24 SE Asia Standard Time</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>11:09:24</td>\n",
       "      <td>700</td>\n",
       "      <td>1534735461826334720</td>\n",
       "      <td>andrawidani</td>\n",
       "      <td>Ni Andrawida</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7893,113.9213,500km</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1620273015232806912</td>\n",
       "      <td>1620273015232806912</td>\n",
       "      <td>2023-01-31 11:09:24 SE Asia Standard Time</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>11:09:24</td>\n",
       "      <td>700</td>\n",
       "      <td>22256691</td>\n",
       "      <td>ohbulancom</td>\n",
       "      <td>ohbulan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7893,113.9213,500km</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1620273015153127424</td>\n",
       "      <td>1620271504285138945</td>\n",
       "      <td>2023-01-31 11:09:24 SE Asia Standard Time</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>11:09:24</td>\n",
       "      <td>700</td>\n",
       "      <td>4819746117</td>\n",
       "      <td>dexansu</td>\n",
       "      <td>Dex Alc√†ntara üáµüá∏</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7893,113.9213,500km</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'screen_name': 'Bixby201', 'name': 'Bixby120...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1620273015077617668</td>\n",
       "      <td>1620273015077617668</td>\n",
       "      <td>2023-01-31 11:09:24 SE Asia Standard Time</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>11:09:24</td>\n",
       "      <td>700</td>\n",
       "      <td>2574886070</td>\n",
       "      <td>andiniabz</td>\n",
       "      <td>andiniabz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7893,113.9213,500km</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1620273014918242307</td>\n",
       "      <td>1620273014918242307</td>\n",
       "      <td>2023-01-31 11:09:24 SE Asia Standard Time</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>11:09:24</td>\n",
       "      <td>700</td>\n",
       "      <td>1907251195</td>\n",
       "      <td>posmalaysia</td>\n",
       "      <td>Pos Malaysia Berhad</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7893,113.9213,500km</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1620273014205210624</td>\n",
       "      <td>1620047157167538176</td>\n",
       "      <td>2023-01-31 11:09:24 SE Asia Standard Time</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>11:09:24</td>\n",
       "      <td>700</td>\n",
       "      <td>1229018772968595456</td>\n",
       "      <td>nadiaayhy</td>\n",
       "      <td>‚Äén.y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7893,113.9213,500km</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'screen_name': 'khkmhk', 'name': 'hkmhüç£üçü', '...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1620273014205194242</td>\n",
       "      <td>1620273014205194242</td>\n",
       "      <td>2023-01-31 11:09:24 SE Asia Standard Time</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>11:09:24</td>\n",
       "      <td>700</td>\n",
       "      <td>803928343049486336</td>\n",
       "      <td>naaerino</td>\n",
       "      <td>naaaa | semi IA ‚úø</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7893,113.9213,500km</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1620273013064359938</td>\n",
       "      <td>1620273013064359938</td>\n",
       "      <td>2023-01-31 11:09:24 SE Asia Standard Time</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>11:09:24</td>\n",
       "      <td>700</td>\n",
       "      <td>1295020647626600449</td>\n",
       "      <td>mgjkinmalaysia</td>\n",
       "      <td>mgjkinmalaysiaüá≤üáæüáµüá≠</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7893,113.9213,500km</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1620273012569407488</td>\n",
       "      <td>1620272521269628928</td>\n",
       "      <td>2023-01-31 11:09:24 SE Asia Standard Time</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>11:09:24</td>\n",
       "      <td>700</td>\n",
       "      <td>1485304128893231107</td>\n",
       "      <td>skies_sg</td>\n",
       "      <td>Skies</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7893,113.9213,500km</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'screen_name': 'rubbeeduckee', 'name': 'lb',...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1620273011965440000</td>\n",
       "      <td>1620273011965440000</td>\n",
       "      <td>2023-01-31 11:09:23 SE Asia Standard Time</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>11:09:23</td>\n",
       "      <td>700</td>\n",
       "      <td>2949715544</td>\n",
       "      <td>hansushii</td>\n",
       "      <td>ŸáÿßŸÜŸä</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7893,113.9213,500km</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1620273008723259392</td>\n",
       "      <td>1620261119389753344</td>\n",
       "      <td>2023-01-31 11:09:23 SE Asia Standard Time</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>11:09:23</td>\n",
       "      <td>700</td>\n",
       "      <td>267678262</td>\n",
       "      <td>mat_din7</td>\n",
       "      <td>DeenuLhaQ üá≤üáæ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7893,113.9213,500km</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'screen_name': 'ammerekak', 'name': 'daddy',...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1620273005380395008</td>\n",
       "      <td>1620271798083551233</td>\n",
       "      <td>2023-01-31 11:09:22 SE Asia Standard Time</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>11:09:22</td>\n",
       "      <td>700</td>\n",
       "      <td>1358082316321529856</td>\n",
       "      <td>itsibrhm</td>\n",
       "      <td>illumiü•Ä</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7893,113.9213,500km</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'screen_name': 'adwinnnn', 'name': '~ vrtcl ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1620273004927414272</td>\n",
       "      <td>1620258285436932101</td>\n",
       "      <td>2023-01-31 11:09:22 SE Asia Standard Time</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>11:09:22</td>\n",
       "      <td>700</td>\n",
       "      <td>904121498</td>\n",
       "      <td>bangboril_</td>\n",
       "      <td>Wawan Kurniawan KM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7893,113.9213,500km</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'screen_name': 'ditaprillia_', 'name': 'ùììùì≤ùìΩùì™...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1620273003383898112</td>\n",
       "      <td>1620273003383898112</td>\n",
       "      <td>2023-01-31 11:09:21 SE Asia Standard Time</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>11:09:21</td>\n",
       "      <td>700</td>\n",
       "      <td>69038358</td>\n",
       "      <td>craigansibin</td>\n",
       "      <td>Giarc Nibisna üá≤üáæüå∫</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7893,113.9213,500km</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1620273001525841920</td>\n",
       "      <td>1620273001525841920</td>\n",
       "      <td>2023-01-31 11:09:21 SE Asia Standard Time</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>11:09:21</td>\n",
       "      <td>700</td>\n",
       "      <td>1644931327</td>\n",
       "      <td>ebidwoles</td>\n",
       "      <td>E_B_I_D :) :) :)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7893,113.9213,500km</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1620273000812810241</td>\n",
       "      <td>1620273000812810241</td>\n",
       "      <td>2023-01-31 11:09:21 SE Asia Standard Time</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>11:09:21</td>\n",
       "      <td>700</td>\n",
       "      <td>236350830</td>\n",
       "      <td>kakrons</td>\n",
       "      <td>kakron</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7893,113.9213,500km</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1620273000082984960</td>\n",
       "      <td>1620273000082984960</td>\n",
       "      <td>2023-01-31 11:09:21 SE Asia Standard Time</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>11:09:21</td>\n",
       "      <td>700</td>\n",
       "      <td>1183648888210608129</td>\n",
       "      <td>midowoolicious</td>\n",
       "      <td>Midoüçì</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7893,113.9213,500km</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1620272998677901318</td>\n",
       "      <td>1620267905991704576</td>\n",
       "      <td>2023-01-31 11:09:20 SE Asia Standard Time</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>11:09:20</td>\n",
       "      <td>700</td>\n",
       "      <td>161244279</td>\n",
       "      <td>syakiraakim</td>\n",
       "      <td>S.A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7893,113.9213,500km</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'screen_name': 'azrulizard', 'name': 'Azrul'...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1620272996874326018</td>\n",
       "      <td>1620272996874326018</td>\n",
       "      <td>2023-01-31 11:09:20 SE Asia Standard Time</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>11:09:20</td>\n",
       "      <td>700</td>\n",
       "      <td>1296260180221366273</td>\n",
       "      <td>liltlegirlpapa</td>\n",
       "      <td>i'mtoxic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7893,113.9213,500km</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1620272996148707333</td>\n",
       "      <td>1620272996148707333</td>\n",
       "      <td>2023-01-31 11:09:20 SE Asia Standard Time</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>11:09:20</td>\n",
       "      <td>700</td>\n",
       "      <td>1490891077435686912</td>\n",
       "      <td>rizkis393</td>\n",
       "      <td>Salmando.io</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7893,113.9213,500km</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1620272995205025797</td>\n",
       "      <td>1620237022257610752</td>\n",
       "      <td>2023-01-31 11:09:19 SE Asia Standard Time</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>11:09:19</td>\n",
       "      <td>700</td>\n",
       "      <td>209623896</td>\n",
       "      <td>azfarza</td>\n",
       "      <td>Azfar Zainal Abidin ü™êüè¥</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7893,113.9213,500km</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'screen_name': 'anthologyve', 'name': 'Yve',...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1620272991849549826</td>\n",
       "      <td>1620272651783766016</td>\n",
       "      <td>2023-01-31 11:09:19 SE Asia Standard Time</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>11:09:19</td>\n",
       "      <td>700</td>\n",
       "      <td>1489828542</td>\n",
       "      <td>just_khan96</td>\n",
       "      <td>Salman Khan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7893,113.9213,500km</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1620272988347338752</td>\n",
       "      <td>1620238814638936064</td>\n",
       "      <td>2023-01-31 11:09:18 SE Asia Standard Time</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>11:09:18</td>\n",
       "      <td>700</td>\n",
       "      <td>133433992</td>\n",
       "      <td>i_am_gunjan</td>\n",
       "      <td>Bearded Biker</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7893,113.9213,500km</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'screen_name': 'TheRadFactor', 'name': 'RK',...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1620272986719936513</td>\n",
       "      <td>1620272986719936513</td>\n",
       "      <td>2023-01-31 11:09:17 SE Asia Standard Time</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>11:09:17</td>\n",
       "      <td>700</td>\n",
       "      <td>82299136</td>\n",
       "      <td>mychinapress</td>\n",
       "      <td>È©¨Êù•Ë•ø‰∫ö‰∏≠ÂõΩÊä•</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7893,113.9213,500km</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1620272985591660545</td>\n",
       "      <td>1620272985591660545</td>\n",
       "      <td>2023-01-31 11:09:17 SE Asia Standard Time</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>11:09:17</td>\n",
       "      <td>700</td>\n",
       "      <td>1195394186456887298</td>\n",
       "      <td>officiallnishaa</td>\n",
       "      <td>‚ô•Ô∏é</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7893,113.9213,500km</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1620272985092558849</td>\n",
       "      <td>1620230993599950849</td>\n",
       "      <td>2023-01-31 11:09:17 SE Asia Standard Time</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>11:09:17</td>\n",
       "      <td>700</td>\n",
       "      <td>1422062175137370112</td>\n",
       "      <td>angel_winnn</td>\n",
       "      <td>üáÆüá©AgustinaüáÆüá©</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7893,113.9213,500km</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'screen_name': 'taeboukie', 'name': 'CJ üåô', ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1620272984484360193</td>\n",
       "      <td>1620272984484360193</td>\n",
       "      <td>2023-01-31 11:09:17 SE Asia Standard Time</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>11:09:17</td>\n",
       "      <td>700</td>\n",
       "      <td>875412271</td>\n",
       "      <td>juang_news</td>\n",
       "      <td>juangnews.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7893,113.9213,500km</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1620272983817478144</td>\n",
       "      <td>1620272983817478144</td>\n",
       "      <td>2023-01-31 11:09:17 SE Asia Standard Time</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>11:09:17</td>\n",
       "      <td>700</td>\n",
       "      <td>1185890789127155712</td>\n",
       "      <td>siikimprit</td>\n",
       "      <td>ale'x bin abi khaleeb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7893,113.9213,500km</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1620272981082767360</td>\n",
       "      <td>1620272981082767360</td>\n",
       "      <td>2023-01-31 11:09:16 SE Asia Standard Time</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>11:09:16</td>\n",
       "      <td>700</td>\n",
       "      <td>417507929</td>\n",
       "      <td>mawwannwan</td>\n",
       "      <td>Squicchi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7893,113.9213,500km</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1620272981045055489</td>\n",
       "      <td>1620272981045055489</td>\n",
       "      <td>2023-01-31 11:09:16 SE Asia Standard Time</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>11:09:16</td>\n",
       "      <td>700</td>\n",
       "      <td>1056969866089979904</td>\n",
       "      <td>shaifulizzuddin</td>\n",
       "      <td>Din</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7893,113.9213,500km</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1620272979866431491</td>\n",
       "      <td>1620270005643509760</td>\n",
       "      <td>2023-01-31 11:09:16 SE Asia Standard Time</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>11:09:16</td>\n",
       "      <td>700</td>\n",
       "      <td>1026304089393483776</td>\n",
       "      <td>iamahdrine</td>\n",
       "      <td>A L E E üí´</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7893,113.9213,500km</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'screen_name': 'svlee29s__', 'name': 'savy (...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1620272979795124225</td>\n",
       "      <td>1620047213916467201</td>\n",
       "      <td>2023-01-31 11:09:16 SE Asia Standard Time</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>11:09:16</td>\n",
       "      <td>700</td>\n",
       "      <td>1010166185294323713</td>\n",
       "      <td>zua_al_azmi</td>\n",
       "      <td>Daddy Afqar</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7893,113.9213,500km</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'screen_name': 'ayiecun', 'name': 'Ayie', 'i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1620272979262464000</td>\n",
       "      <td>1620253783992471552</td>\n",
       "      <td>2023-01-31 11:09:16 SE Asia Standard Time</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>11:09:16</td>\n",
       "      <td>700</td>\n",
       "      <td>1485304128893231107</td>\n",
       "      <td>skies_sg</td>\n",
       "      <td>Skies</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7893,113.9213,500km</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'screen_name': 'cloudykh', 'name': 'Ÿã', 'id'...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1620272978301964288</td>\n",
       "      <td>1620272978301964288</td>\n",
       "      <td>2023-01-31 11:09:15 SE Asia Standard Time</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>11:09:15</td>\n",
       "      <td>700</td>\n",
       "      <td>41022830</td>\n",
       "      <td>einhzrn</td>\n",
       "      <td>Ben Hzrn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7893,113.9213,500km</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1620272974254456832</td>\n",
       "      <td>1620272974254456832</td>\n",
       "      <td>2023-01-31 11:09:14 SE Asia Standard Time</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>11:09:14</td>\n",
       "      <td>700</td>\n",
       "      <td>1481114733059411969</td>\n",
       "      <td>haexuxxieeejja</td>\n",
       "      <td>i met wayvdreamies twice ‚ô°/ot26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7893,113.9213,500km</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1620272974090887168</td>\n",
       "      <td>1620272974090887168</td>\n",
       "      <td>2023-01-31 11:09:14 SE Asia Standard Time</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>11:09:14</td>\n",
       "      <td>700</td>\n",
       "      <td>1287763261262188545</td>\n",
       "      <td>praveen11849775</td>\n",
       "      <td>Ê≤πÁÆ°YouTubeÂà∑Á≤â‰∏ù‰π∞ËÆ¢ÈòÖÂà∑Êí≠ÊîæÈáèÂä†ÁÇπËµû‰π∞Êó∂Èïø Êé®ÁâπÂà∑Á≤â‰∏ù‰π∞Á≤â‰∏ùÂà∑ÁÇπËµû‰∏äÁÉ≠Èó®</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7893,113.9213,500km</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1620272973470126080</td>\n",
       "      <td>1620198093089308672</td>\n",
       "      <td>2023-01-31 11:09:14 SE Asia Standard Time</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>11:09:14</td>\n",
       "      <td>700</td>\n",
       "      <td>328061512</td>\n",
       "      <td>menejerkaravan</td>\n",
       "      <td>Menejer Karavan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7893,113.9213,500km</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'screen_name': 'Tuahsakato999', 'name': 'Ver...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1620272973449154561</td>\n",
       "      <td>1620254289737453570</td>\n",
       "      <td>2023-01-31 11:09:14 SE Asia Standard Time</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>11:09:14</td>\n",
       "      <td>700</td>\n",
       "      <td>1347448104874741760</td>\n",
       "      <td>danangbiy</td>\n",
       "      <td>Om Theodore</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7893,113.9213,500km</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'screen_name': 'baejaneee', 'name': 'ü•Äü•Äü•Äü•Ä', ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1620272973423972353</td>\n",
       "      <td>1620265379863076864</td>\n",
       "      <td>2023-01-31 11:09:14 SE Asia Standard Time</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>11:09:14</td>\n",
       "      <td>700</td>\n",
       "      <td>129140574</td>\n",
       "      <td>adawiahism</td>\n",
       "      <td>ad</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7893,113.9213,500km</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1620272972530585601</td>\n",
       "      <td>1620272972530585601</td>\n",
       "      <td>2023-01-31 11:09:14 SE Asia Standard Time</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>11:09:14</td>\n",
       "      <td>700</td>\n",
       "      <td>1224698401058476034</td>\n",
       "      <td>myedisi9</td>\n",
       "      <td>Edisi 9 Official</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7893,113.9213,500km</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1620272972086022144</td>\n",
       "      <td>1619994176720109569</td>\n",
       "      <td>2023-01-31 11:09:14 SE Asia Standard Time</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>11:09:14</td>\n",
       "      <td>700</td>\n",
       "      <td>238261138</td>\n",
       "      <td>firdaussmazlan</td>\n",
       "      <td>Muhd Firdaus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7893,113.9213,500km</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'screen_name': 'zeleyla', 'name': 'Leyla Z üè¥...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1620272971482030080</td>\n",
       "      <td>1620263928411611136</td>\n",
       "      <td>2023-01-31 11:09:14 SE Asia Standard Time</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>11:09:14</td>\n",
       "      <td>700</td>\n",
       "      <td>1081079950570975232</td>\n",
       "      <td>lovenajaemin13</td>\n",
       "      <td>üòªNOONA JAEMIN JENOüê∂</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7893,113.9213,500km</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1620272970806759425</td>\n",
       "      <td>1620237517277786113</td>\n",
       "      <td>2023-01-31 11:09:14 SE Asia Standard Time</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>11:09:14</td>\n",
       "      <td>700</td>\n",
       "      <td>1480936716848361474</td>\n",
       "      <td>pioner0ld_pi</td>\n",
       "      <td>Qudsss</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7893,113.9213,500km</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'screen_name': 'YENIRIANTO2', 'name': 'YENIR...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1620272969368100864</td>\n",
       "      <td>1620272969368100864</td>\n",
       "      <td>2023-01-31 11:09:13 SE Asia Standard Time</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>11:09:13</td>\n",
       "      <td>700</td>\n",
       "      <td>125075205</td>\n",
       "      <td>kewayamin</td>\n",
       "      <td>ÿÆŸäÿ±ÿßŸÑÿ£ŸÜŸàÿßÿ± ŸäŸÖŸäŸÜ.Ÿ©Ÿ°Ÿ°Ÿ†Ÿ†Ÿ¶</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7893,113.9213,500km</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows √ó 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id      conversation_id  \\\n",
       "0   1620273021507309568  1620273021507309568   \n",
       "1   1620273019678785537  1620273019678785537   \n",
       "2   1620273019305492480  1620273014205194242   \n",
       "3   1620273018898624514  1620258168898199554   \n",
       "4   1620273018038808583  1620270912854724610   \n",
       "5   1620273017627766786  1620242295319842819   \n",
       "6   1620273016084250624  1620273016084250624   \n",
       "7   1620273015232806912  1620273015232806912   \n",
       "8   1620273015153127424  1620271504285138945   \n",
       "9   1620273015077617668  1620273015077617668   \n",
       "10  1620273014918242307  1620273014918242307   \n",
       "11  1620273014205210624  1620047157167538176   \n",
       "12  1620273014205194242  1620273014205194242   \n",
       "13  1620273013064359938  1620273013064359938   \n",
       "14  1620273012569407488  1620272521269628928   \n",
       "15  1620273011965440000  1620273011965440000   \n",
       "16  1620273008723259392  1620261119389753344   \n",
       "17  1620273005380395008  1620271798083551233   \n",
       "18  1620273004927414272  1620258285436932101   \n",
       "19  1620273003383898112  1620273003383898112   \n",
       "20  1620273001525841920  1620273001525841920   \n",
       "21  1620273000812810241  1620273000812810241   \n",
       "22  1620273000082984960  1620273000082984960   \n",
       "23  1620272998677901318  1620267905991704576   \n",
       "24  1620272996874326018  1620272996874326018   \n",
       "25  1620272996148707333  1620272996148707333   \n",
       "26  1620272995205025797  1620237022257610752   \n",
       "27  1620272991849549826  1620272651783766016   \n",
       "28  1620272988347338752  1620238814638936064   \n",
       "29  1620272986719936513  1620272986719936513   \n",
       "30  1620272985591660545  1620272985591660545   \n",
       "31  1620272985092558849  1620230993599950849   \n",
       "32  1620272984484360193  1620272984484360193   \n",
       "33  1620272983817478144  1620272983817478144   \n",
       "34  1620272981082767360  1620272981082767360   \n",
       "35  1620272981045055489  1620272981045055489   \n",
       "36  1620272979866431491  1620270005643509760   \n",
       "37  1620272979795124225  1620047213916467201   \n",
       "38  1620272979262464000  1620253783992471552   \n",
       "39  1620272978301964288  1620272978301964288   \n",
       "40  1620272974254456832  1620272974254456832   \n",
       "41  1620272974090887168  1620272974090887168   \n",
       "42  1620272973470126080  1620198093089308672   \n",
       "43  1620272973449154561  1620254289737453570   \n",
       "44  1620272973423972353  1620265379863076864   \n",
       "45  1620272972530585601  1620272972530585601   \n",
       "46  1620272972086022144  1619994176720109569   \n",
       "47  1620272971482030080  1620263928411611136   \n",
       "48  1620272970806759425  1620237517277786113   \n",
       "49  1620272969368100864  1620272969368100864   \n",
       "\n",
       "                                   created_at        date      time  timezone  \\\n",
       "0   2023-01-31 11:09:26 SE Asia Standard Time  2023-01-31  11:09:26       700   \n",
       "1   2023-01-31 11:09:25 SE Asia Standard Time  2023-01-31  11:09:25       700   \n",
       "2   2023-01-31 11:09:25 SE Asia Standard Time  2023-01-31  11:09:25       700   \n",
       "3   2023-01-31 11:09:25 SE Asia Standard Time  2023-01-31  11:09:25       700   \n",
       "4   2023-01-31 11:09:25 SE Asia Standard Time  2023-01-31  11:09:25       700   \n",
       "5   2023-01-31 11:09:25 SE Asia Standard Time  2023-01-31  11:09:25       700   \n",
       "6   2023-01-31 11:09:24 SE Asia Standard Time  2023-01-31  11:09:24       700   \n",
       "7   2023-01-31 11:09:24 SE Asia Standard Time  2023-01-31  11:09:24       700   \n",
       "8   2023-01-31 11:09:24 SE Asia Standard Time  2023-01-31  11:09:24       700   \n",
       "9   2023-01-31 11:09:24 SE Asia Standard Time  2023-01-31  11:09:24       700   \n",
       "10  2023-01-31 11:09:24 SE Asia Standard Time  2023-01-31  11:09:24       700   \n",
       "11  2023-01-31 11:09:24 SE Asia Standard Time  2023-01-31  11:09:24       700   \n",
       "12  2023-01-31 11:09:24 SE Asia Standard Time  2023-01-31  11:09:24       700   \n",
       "13  2023-01-31 11:09:24 SE Asia Standard Time  2023-01-31  11:09:24       700   \n",
       "14  2023-01-31 11:09:24 SE Asia Standard Time  2023-01-31  11:09:24       700   \n",
       "15  2023-01-31 11:09:23 SE Asia Standard Time  2023-01-31  11:09:23       700   \n",
       "16  2023-01-31 11:09:23 SE Asia Standard Time  2023-01-31  11:09:23       700   \n",
       "17  2023-01-31 11:09:22 SE Asia Standard Time  2023-01-31  11:09:22       700   \n",
       "18  2023-01-31 11:09:22 SE Asia Standard Time  2023-01-31  11:09:22       700   \n",
       "19  2023-01-31 11:09:21 SE Asia Standard Time  2023-01-31  11:09:21       700   \n",
       "20  2023-01-31 11:09:21 SE Asia Standard Time  2023-01-31  11:09:21       700   \n",
       "21  2023-01-31 11:09:21 SE Asia Standard Time  2023-01-31  11:09:21       700   \n",
       "22  2023-01-31 11:09:21 SE Asia Standard Time  2023-01-31  11:09:21       700   \n",
       "23  2023-01-31 11:09:20 SE Asia Standard Time  2023-01-31  11:09:20       700   \n",
       "24  2023-01-31 11:09:20 SE Asia Standard Time  2023-01-31  11:09:20       700   \n",
       "25  2023-01-31 11:09:20 SE Asia Standard Time  2023-01-31  11:09:20       700   \n",
       "26  2023-01-31 11:09:19 SE Asia Standard Time  2023-01-31  11:09:19       700   \n",
       "27  2023-01-31 11:09:19 SE Asia Standard Time  2023-01-31  11:09:19       700   \n",
       "28  2023-01-31 11:09:18 SE Asia Standard Time  2023-01-31  11:09:18       700   \n",
       "29  2023-01-31 11:09:17 SE Asia Standard Time  2023-01-31  11:09:17       700   \n",
       "30  2023-01-31 11:09:17 SE Asia Standard Time  2023-01-31  11:09:17       700   \n",
       "31  2023-01-31 11:09:17 SE Asia Standard Time  2023-01-31  11:09:17       700   \n",
       "32  2023-01-31 11:09:17 SE Asia Standard Time  2023-01-31  11:09:17       700   \n",
       "33  2023-01-31 11:09:17 SE Asia Standard Time  2023-01-31  11:09:17       700   \n",
       "34  2023-01-31 11:09:16 SE Asia Standard Time  2023-01-31  11:09:16       700   \n",
       "35  2023-01-31 11:09:16 SE Asia Standard Time  2023-01-31  11:09:16       700   \n",
       "36  2023-01-31 11:09:16 SE Asia Standard Time  2023-01-31  11:09:16       700   \n",
       "37  2023-01-31 11:09:16 SE Asia Standard Time  2023-01-31  11:09:16       700   \n",
       "38  2023-01-31 11:09:16 SE Asia Standard Time  2023-01-31  11:09:16       700   \n",
       "39  2023-01-31 11:09:15 SE Asia Standard Time  2023-01-31  11:09:15       700   \n",
       "40  2023-01-31 11:09:14 SE Asia Standard Time  2023-01-31  11:09:14       700   \n",
       "41  2023-01-31 11:09:14 SE Asia Standard Time  2023-01-31  11:09:14       700   \n",
       "42  2023-01-31 11:09:14 SE Asia Standard Time  2023-01-31  11:09:14       700   \n",
       "43  2023-01-31 11:09:14 SE Asia Standard Time  2023-01-31  11:09:14       700   \n",
       "44  2023-01-31 11:09:14 SE Asia Standard Time  2023-01-31  11:09:14       700   \n",
       "45  2023-01-31 11:09:14 SE Asia Standard Time  2023-01-31  11:09:14       700   \n",
       "46  2023-01-31 11:09:14 SE Asia Standard Time  2023-01-31  11:09:14       700   \n",
       "47  2023-01-31 11:09:14 SE Asia Standard Time  2023-01-31  11:09:14       700   \n",
       "48  2023-01-31 11:09:14 SE Asia Standard Time  2023-01-31  11:09:14       700   \n",
       "49  2023-01-31 11:09:13 SE Asia Standard Time  2023-01-31  11:09:13       700   \n",
       "\n",
       "                user_id         username  \\\n",
       "0             600050060        juescha86   \n",
       "1            1399762345       yazlijufri   \n",
       "2    803928343049486336         naaerino   \n",
       "3            3093417654           zapyy0   \n",
       "4              71463675      junchgaming   \n",
       "5   1166573925033218049  tujuhderajat0_0   \n",
       "6   1534735461826334720      andrawidani   \n",
       "7              22256691       ohbulancom   \n",
       "8            4819746117          dexansu   \n",
       "9            2574886070        andiniabz   \n",
       "10           1907251195      posmalaysia   \n",
       "11  1229018772968595456        nadiaayhy   \n",
       "12   803928343049486336         naaerino   \n",
       "13  1295020647626600449   mgjkinmalaysia   \n",
       "14  1485304128893231107         skies_sg   \n",
       "15           2949715544        hansushii   \n",
       "16            267678262         mat_din7   \n",
       "17  1358082316321529856         itsibrhm   \n",
       "18            904121498       bangboril_   \n",
       "19             69038358     craigansibin   \n",
       "20           1644931327        ebidwoles   \n",
       "21            236350830          kakrons   \n",
       "22  1183648888210608129   midowoolicious   \n",
       "23            161244279      syakiraakim   \n",
       "24  1296260180221366273   liltlegirlpapa   \n",
       "25  1490891077435686912        rizkis393   \n",
       "26            209623896          azfarza   \n",
       "27           1489828542      just_khan96   \n",
       "28            133433992      i_am_gunjan   \n",
       "29             82299136     mychinapress   \n",
       "30  1195394186456887298  officiallnishaa   \n",
       "31  1422062175137370112      angel_winnn   \n",
       "32            875412271       juang_news   \n",
       "33  1185890789127155712       siikimprit   \n",
       "34            417507929       mawwannwan   \n",
       "35  1056969866089979904  shaifulizzuddin   \n",
       "36  1026304089393483776       iamahdrine   \n",
       "37  1010166185294323713      zua_al_azmi   \n",
       "38  1485304128893231107         skies_sg   \n",
       "39             41022830          einhzrn   \n",
       "40  1481114733059411969   haexuxxieeejja   \n",
       "41  1287763261262188545  praveen11849775   \n",
       "42            328061512   menejerkaravan   \n",
       "43  1347448104874741760        danangbiy   \n",
       "44            129140574       adawiahism   \n",
       "45  1224698401058476034         myedisi9   \n",
       "46            238261138   firdaussmazlan   \n",
       "47  1081079950570975232   lovenajaemin13   \n",
       "48  1480936716848361474     pioner0ld_pi   \n",
       "49            125075205        kewayamin   \n",
       "\n",
       "                                        name place  ...  \\\n",
       "0        ÿ≠ÿµŸá ÿ™ÿßŸÖÿ± ‚ó•‚Ä¢ÔªúŸïŸàŸêÿ®ŸàŸÜ‚Ä¢ ‚ÄèÔ∫ßŸïŸìÿµŸçŸÖŸç‚Ä¢ÔªúŸïŸàŸêÿØŸì   NaN  ...   \n",
       "1                                  Jufri Jup   NaN  ...   \n",
       "2                          naaaa | semi IA ‚úø   NaN  ...   \n",
       "3                                        zap   NaN  ...   \n",
       "4                  Junch is Taipei Game Show   NaN  ...   \n",
       "5                                  Lindemann   NaN  ...   \n",
       "6                               Ni Andrawida   NaN  ...   \n",
       "7                                    ohbulan   NaN  ...   \n",
       "8                           Dex Alc√†ntara üáµüá∏   NaN  ...   \n",
       "9                                  andiniabz   NaN  ...   \n",
       "10                       Pos Malaysia Berhad   NaN  ...   \n",
       "11                                      ‚Äén.y   NaN  ...   \n",
       "12                         naaaa | semi IA ‚úø   NaN  ...   \n",
       "13                        mgjkinmalaysiaüá≤üáæüáµüá≠   NaN  ...   \n",
       "14                                     Skies   NaN  ...   \n",
       "15                                      ŸáÿßŸÜŸä   NaN  ...   \n",
       "16                              DeenuLhaQ üá≤üáæ   NaN  ...   \n",
       "17                                   illumiü•Ä   NaN  ...   \n",
       "18                        Wawan Kurniawan KM   NaN  ...   \n",
       "19                         Giarc Nibisna üá≤üáæüå∫   NaN  ...   \n",
       "20                          E_B_I_D :) :) :)   NaN  ...   \n",
       "21                                    kakron   NaN  ...   \n",
       "22                                     Midoüçì   NaN  ...   \n",
       "23                                       S.A   NaN  ...   \n",
       "24                                  i'mtoxic   NaN  ...   \n",
       "25                               Salmando.io   NaN  ...   \n",
       "26                    Azfar Zainal Abidin ü™êüè¥   NaN  ...   \n",
       "27                               Salman Khan   NaN  ...   \n",
       "28                             Bearded Biker   NaN  ...   \n",
       "29                                   È©¨Êù•Ë•ø‰∫ö‰∏≠ÂõΩÊä•   NaN  ...   \n",
       "30                                        ‚ô•Ô∏é   NaN  ...   \n",
       "31                              üáÆüá©AgustinaüáÆüá©   NaN  ...   \n",
       "32                             juangnews.com   NaN  ...   \n",
       "33                     ale'x bin abi khaleeb   NaN  ...   \n",
       "34                                  Squicchi   NaN  ...   \n",
       "35                                       Din   NaN  ...   \n",
       "36                                 A L E E üí´   NaN  ...   \n",
       "37                               Daddy Afqar   NaN  ...   \n",
       "38                                     Skies   NaN  ...   \n",
       "39                                  Ben Hzrn   NaN  ...   \n",
       "40           i met wayvdreamies twice ‚ô°/ot26   NaN  ...   \n",
       "41  Ê≤πÁÆ°YouTubeÂà∑Á≤â‰∏ù‰π∞ËÆ¢ÈòÖÂà∑Êí≠ÊîæÈáèÂä†ÁÇπËµû‰π∞Êó∂Èïø Êé®ÁâπÂà∑Á≤â‰∏ù‰π∞Á≤â‰∏ùÂà∑ÁÇπËµû‰∏äÁÉ≠Èó®   NaN  ...   \n",
       "42                           Menejer Karavan   NaN  ...   \n",
       "43                               Om Theodore   NaN  ...   \n",
       "44                                        ad   NaN  ...   \n",
       "45                          Edisi 9 Official   NaN  ...   \n",
       "46                              Muhd Firdaus   NaN  ...   \n",
       "47                       üòªNOONA JAEMIN JENOüê∂   NaN  ...   \n",
       "48                                    Qudsss   NaN  ...   \n",
       "49                    ÿÆŸäÿ±ÿßŸÑÿ£ŸÜŸàÿßÿ± ŸäŸÖŸäŸÜ.Ÿ©Ÿ°Ÿ°Ÿ†Ÿ†Ÿ¶   NaN  ...   \n",
       "\n",
       "                      geo source user_rt_id user_rt retweet_id  \\\n",
       "0   0.7893,113.9213,500km    NaN        NaN     NaN        NaN   \n",
       "1   0.7893,113.9213,500km    NaN        NaN     NaN        NaN   \n",
       "2   0.7893,113.9213,500km    NaN        NaN     NaN        NaN   \n",
       "3   0.7893,113.9213,500km    NaN        NaN     NaN        NaN   \n",
       "4   0.7893,113.9213,500km    NaN        NaN     NaN        NaN   \n",
       "5   0.7893,113.9213,500km    NaN        NaN     NaN        NaN   \n",
       "6   0.7893,113.9213,500km    NaN        NaN     NaN        NaN   \n",
       "7   0.7893,113.9213,500km    NaN        NaN     NaN        NaN   \n",
       "8   0.7893,113.9213,500km    NaN        NaN     NaN        NaN   \n",
       "9   0.7893,113.9213,500km    NaN        NaN     NaN        NaN   \n",
       "10  0.7893,113.9213,500km    NaN        NaN     NaN        NaN   \n",
       "11  0.7893,113.9213,500km    NaN        NaN     NaN        NaN   \n",
       "12  0.7893,113.9213,500km    NaN        NaN     NaN        NaN   \n",
       "13  0.7893,113.9213,500km    NaN        NaN     NaN        NaN   \n",
       "14  0.7893,113.9213,500km    NaN        NaN     NaN        NaN   \n",
       "15  0.7893,113.9213,500km    NaN        NaN     NaN        NaN   \n",
       "16  0.7893,113.9213,500km    NaN        NaN     NaN        NaN   \n",
       "17  0.7893,113.9213,500km    NaN        NaN     NaN        NaN   \n",
       "18  0.7893,113.9213,500km    NaN        NaN     NaN        NaN   \n",
       "19  0.7893,113.9213,500km    NaN        NaN     NaN        NaN   \n",
       "20  0.7893,113.9213,500km    NaN        NaN     NaN        NaN   \n",
       "21  0.7893,113.9213,500km    NaN        NaN     NaN        NaN   \n",
       "22  0.7893,113.9213,500km    NaN        NaN     NaN        NaN   \n",
       "23  0.7893,113.9213,500km    NaN        NaN     NaN        NaN   \n",
       "24  0.7893,113.9213,500km    NaN        NaN     NaN        NaN   \n",
       "25  0.7893,113.9213,500km    NaN        NaN     NaN        NaN   \n",
       "26  0.7893,113.9213,500km    NaN        NaN     NaN        NaN   \n",
       "27  0.7893,113.9213,500km    NaN        NaN     NaN        NaN   \n",
       "28  0.7893,113.9213,500km    NaN        NaN     NaN        NaN   \n",
       "29  0.7893,113.9213,500km    NaN        NaN     NaN        NaN   \n",
       "30  0.7893,113.9213,500km    NaN        NaN     NaN        NaN   \n",
       "31  0.7893,113.9213,500km    NaN        NaN     NaN        NaN   \n",
       "32  0.7893,113.9213,500km    NaN        NaN     NaN        NaN   \n",
       "33  0.7893,113.9213,500km    NaN        NaN     NaN        NaN   \n",
       "34  0.7893,113.9213,500km    NaN        NaN     NaN        NaN   \n",
       "35  0.7893,113.9213,500km    NaN        NaN     NaN        NaN   \n",
       "36  0.7893,113.9213,500km    NaN        NaN     NaN        NaN   \n",
       "37  0.7893,113.9213,500km    NaN        NaN     NaN        NaN   \n",
       "38  0.7893,113.9213,500km    NaN        NaN     NaN        NaN   \n",
       "39  0.7893,113.9213,500km    NaN        NaN     NaN        NaN   \n",
       "40  0.7893,113.9213,500km    NaN        NaN     NaN        NaN   \n",
       "41  0.7893,113.9213,500km    NaN        NaN     NaN        NaN   \n",
       "42  0.7893,113.9213,500km    NaN        NaN     NaN        NaN   \n",
       "43  0.7893,113.9213,500km    NaN        NaN     NaN        NaN   \n",
       "44  0.7893,113.9213,500km    NaN        NaN     NaN        NaN   \n",
       "45  0.7893,113.9213,500km    NaN        NaN     NaN        NaN   \n",
       "46  0.7893,113.9213,500km    NaN        NaN     NaN        NaN   \n",
       "47  0.7893,113.9213,500km    NaN        NaN     NaN        NaN   \n",
       "48  0.7893,113.9213,500km    NaN        NaN     NaN        NaN   \n",
       "49  0.7893,113.9213,500km    NaN        NaN     NaN        NaN   \n",
       "\n",
       "                                             reply_to  retweet_date  \\\n",
       "0                                                  []           NaN   \n",
       "1                                                  []           NaN   \n",
       "2                                                  []           NaN   \n",
       "3   [{'screen_name': 'hrthhrth', 'name': 'H', 'id'...           NaN   \n",
       "4   [{'screen_name': 'personatheorymy', 'name': 'P...           NaN   \n",
       "5   [{'screen_name': 'KomediRetjeh', 'name': 'Kome...           NaN   \n",
       "6                                                  []           NaN   \n",
       "7                                                  []           NaN   \n",
       "8   [{'screen_name': 'Bixby201', 'name': 'Bixby120...           NaN   \n",
       "9                                                  []           NaN   \n",
       "10                                                 []           NaN   \n",
       "11  [{'screen_name': 'khkmhk', 'name': 'hkmhüç£üçü', '...           NaN   \n",
       "12                                                 []           NaN   \n",
       "13                                                 []           NaN   \n",
       "14  [{'screen_name': 'rubbeeduckee', 'name': 'lb',...           NaN   \n",
       "15                                                 []           NaN   \n",
       "16  [{'screen_name': 'ammerekak', 'name': 'daddy',...           NaN   \n",
       "17  [{'screen_name': 'adwinnnn', 'name': '~ vrtcl ...           NaN   \n",
       "18  [{'screen_name': 'ditaprillia_', 'name': 'ùììùì≤ùìΩùì™...           NaN   \n",
       "19                                                 []           NaN   \n",
       "20                                                 []           NaN   \n",
       "21                                                 []           NaN   \n",
       "22                                                 []           NaN   \n",
       "23  [{'screen_name': 'azrulizard', 'name': 'Azrul'...           NaN   \n",
       "24                                                 []           NaN   \n",
       "25                                                 []           NaN   \n",
       "26  [{'screen_name': 'anthologyve', 'name': 'Yve',...           NaN   \n",
       "27                                                 []           NaN   \n",
       "28  [{'screen_name': 'TheRadFactor', 'name': 'RK',...           NaN   \n",
       "29                                                 []           NaN   \n",
       "30                                                 []           NaN   \n",
       "31  [{'screen_name': 'taeboukie', 'name': 'CJ üåô', ...           NaN   \n",
       "32                                                 []           NaN   \n",
       "33                                                 []           NaN   \n",
       "34                                                 []           NaN   \n",
       "35                                                 []           NaN   \n",
       "36  [{'screen_name': 'svlee29s__', 'name': 'savy (...           NaN   \n",
       "37  [{'screen_name': 'ayiecun', 'name': 'Ayie', 'i...           NaN   \n",
       "38  [{'screen_name': 'cloudykh', 'name': 'Ÿã', 'id'...           NaN   \n",
       "39                                                 []           NaN   \n",
       "40                                                 []           NaN   \n",
       "41                                                 []           NaN   \n",
       "42  [{'screen_name': 'Tuahsakato999', 'name': 'Ver...           NaN   \n",
       "43  [{'screen_name': 'baejaneee', 'name': 'ü•Äü•Äü•Äü•Ä', ...           NaN   \n",
       "44                                                 []           NaN   \n",
       "45                                                 []           NaN   \n",
       "46  [{'screen_name': 'zeleyla', 'name': 'Leyla Z üè¥...           NaN   \n",
       "47                                                 []           NaN   \n",
       "48  [{'screen_name': 'YENIRIANTO2', 'name': 'YENIR...           NaN   \n",
       "49                                                 []           NaN   \n",
       "\n",
       "    translate trans_src trans_dest  \n",
       "0         NaN       NaN        NaN  \n",
       "1         NaN       NaN        NaN  \n",
       "2         NaN       NaN        NaN  \n",
       "3         NaN       NaN        NaN  \n",
       "4         NaN       NaN        NaN  \n",
       "5         NaN       NaN        NaN  \n",
       "6         NaN       NaN        NaN  \n",
       "7         NaN       NaN        NaN  \n",
       "8         NaN       NaN        NaN  \n",
       "9         NaN       NaN        NaN  \n",
       "10        NaN       NaN        NaN  \n",
       "11        NaN       NaN        NaN  \n",
       "12        NaN       NaN        NaN  \n",
       "13        NaN       NaN        NaN  \n",
       "14        NaN       NaN        NaN  \n",
       "15        NaN       NaN        NaN  \n",
       "16        NaN       NaN        NaN  \n",
       "17        NaN       NaN        NaN  \n",
       "18        NaN       NaN        NaN  \n",
       "19        NaN       NaN        NaN  \n",
       "20        NaN       NaN        NaN  \n",
       "21        NaN       NaN        NaN  \n",
       "22        NaN       NaN        NaN  \n",
       "23        NaN       NaN        NaN  \n",
       "24        NaN       NaN        NaN  \n",
       "25        NaN       NaN        NaN  \n",
       "26        NaN       NaN        NaN  \n",
       "27        NaN       NaN        NaN  \n",
       "28        NaN       NaN        NaN  \n",
       "29        NaN       NaN        NaN  \n",
       "30        NaN       NaN        NaN  \n",
       "31        NaN       NaN        NaN  \n",
       "32        NaN       NaN        NaN  \n",
       "33        NaN       NaN        NaN  \n",
       "34        NaN       NaN        NaN  \n",
       "35        NaN       NaN        NaN  \n",
       "36        NaN       NaN        NaN  \n",
       "37        NaN       NaN        NaN  \n",
       "38        NaN       NaN        NaN  \n",
       "39        NaN       NaN        NaN  \n",
       "40        NaN       NaN        NaN  \n",
       "41        NaN       NaN        NaN  \n",
       "42        NaN       NaN        NaN  \n",
       "43        NaN       NaN        NaN  \n",
       "44        NaN       NaN        NaN  \n",
       "45        NaN       NaN        NaN  \n",
       "46        NaN       NaN        NaN  \n",
       "47        NaN       NaN        NaN  \n",
       "48        NaN       NaN        NaN  \n",
       "49        NaN       NaN        NaN  \n",
       "\n",
       "[50 rows x 36 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_scraped.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69e7e23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer_factory = StemmerFactory()\n",
    "stemmer = stemmer_factory.create_stemmer()\n",
    "data_scraped = data_scraped['tweet'].apply(stemmer.stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0b4211e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_labelled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\pradi\\emotion classification thesis.ipynb Cell 21\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/pradi/emotion%20classification%20thesis.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m data_labelled \u001b[39m=\u001b[39m data_labelled\u001b[39m.\u001b[39mrename(columns\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mtweet\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m})\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pradi/emotion%20classification%20thesis.ipynb#X26sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m data_labelled_rb \u001b[39m=\u001b[39m rb\u001b[39m.\u001b[39mread_pandas(data_labelled, task\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTextClassification\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_labelled' is not defined"
     ]
    }
   ],
   "source": [
    "data_labelled = data_labelled.rename(columns={\"tweet\": \"text\"})\n",
    "data_labelled_rb = rb.read_pandas(data_labelled, task=\"TextClassification\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9de4dc47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-11 03:00:47.699 | WARNING  | rubrix.client.datasets:from_pandas:341 - Following columns are not supported by the TextClassificationRecord model and are ignored: ['tweet']\n"
     ]
    }
   ],
   "source": [
    "data_scraped2 = data.rename(columns={\"tweet\": \"text\"})\n",
    "data_scraped2_rb = rb.read_pandas(data, task=\"TextClassification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "92ef50d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scraped = data_scraped.rename(columns={\"tweet\": \"text\"})\n",
    "data_scraped_rb = rb.read_pandas(data_scraped, task=\"TextClassification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f52b5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 2567\n"
     ]
    }
   ],
   "source": [
    "training_data = data_scraped_preprocessed.sample(frac=0.19, random_state=25)\n",
    "# testing_data = snorkel_data.drop(training_data.index)\n",
    "\n",
    "print(f\"Number of training examples: {training_data.shape[0]}\")\n",
    "# print(f\"Number of testing examples: {testing_data.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "57f0d2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 3521\n",
      "Number of testing examples: 880\n"
     ]
    }
   ],
   "source": [
    "training_data = data_labelled.sample(frac=0.8, random_state=42)\n",
    "testing_data = data_labelled.drop(training_data.index)\n",
    "\n",
    "print(f\"Number of training examples: {training_data.shape[0]}\")\n",
    "print(f\"Number of testing examples: {testing_data.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb5e0317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 1320\n"
     ]
    }
   ],
   "source": [
    "testing_data_labelled = data_labelled.sample(frac=0.3, random_state=0)\n",
    "# testing_data_labelled = data_labelled.drop(training_data.index)\n",
    "\n",
    "print(f\"Number of testing examples: {testing_data_labelled.shape[0]}\")\n",
    "# print(f\"Number of testing examples: {testing_data.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87c45d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scraped2 = data_scraped2.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cdd52b45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sekarang udah ada union tapiiiiiiiii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>people followed me and one person unfollowed m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sore yopp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Laper yaa mas Wkwwk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oooh can wait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78215</th>\n",
       "      <td>Emang udah paling bener gausah cerita apa sm s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78216</th>\n",
       "      <td>Merosotin cuy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78217</th>\n",
       "      <td>Lahh ternyata ada fans nya yang komen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78218</th>\n",
       "      <td>Employee commercial would rather QQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78219</th>\n",
       "      <td>Gatau ko aku taunya Kontol Kejepit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78220 rows √ó 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tweet\n",
       "0                   Sekarang udah ada union tapiiiiiiiii\n",
       "1      people followed me and one person unfollowed m...\n",
       "2                                              Sore yopp\n",
       "3                                    Laper yaa mas Wkwwk\n",
       "4                                          Oooh can wait\n",
       "...                                                  ...\n",
       "78215  Emang udah paling bener gausah cerita apa sm s...\n",
       "78216                                      Merosotin cuy\n",
       "78217              Lahh ternyata ada fans nya yang komen\n",
       "78218                Employee commercial would rather QQ\n",
       "78219                 Gatau ko aku taunya Kontol Kejepit\n",
       "\n",
       "[78220 rows x 1 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_scraped2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8491eff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\rubrix\\client\\models.py:43: UserWarning: Some metadata values exceed the max length. Those values will be truncated by keeping only the last 128 characters.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6a769d6006a47459b24f50e4fd97984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78220 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78220 records logged to http://localhost:6900/datasets/rubrix/emotion_twitter_v10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BulkResponse(dataset='emotion_twitter_v10', processed=78220, failed=0)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import rubrix as rb\n",
    "# build our training records without labels\n",
    "records = [\n",
    "    rb.TextClassificationRecord(\n",
    "        text=row.tweet,\n",
    "        metadata={\"tweet\":row.tweet},\n",
    "    )\n",
    "    for i,row in data_scraped2.iterrows()\n",
    "]\n",
    "rb.log(records, name=\"emotion_twitter_v10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b0802663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97df263974ce48e8b735321f67b280f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "880 records logged to http://localhost:6900/datasets/rubrix/emotion_twitter_v10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BulkResponse(dataset='emotion_twitter_v10', processed=880, failed=0)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build our labelled records to evaluate our heuristic rules and optimize the thresholds\n",
    "labels = ['happy', 'anger', 'sadness', 'fear', 'love']\n",
    "\n",
    "records = [\n",
    "    rb.TextClassificationRecord(\n",
    "        text=row.tweet,\n",
    "        annotation=labels[row.label],\n",
    "        metadata={\"Tweet\":row.tweet},\n",
    "    )\n",
    "    for i,row in testing_data.iterrows()\n",
    "]\n",
    "\n",
    "# log the records to Rubrix\n",
    "rb.log(records, name=\"emotion_twitter_v10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2271f849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03cc82ded57341ecb13f6abb6d7ceb35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preparing rules:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1455d0bd2ad4422bb793a721199e388c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying rules:   0%|          | 0/79100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rubrix.labeling.text_classification import WeakLabels\n",
    "\n",
    "weak_labels = WeakLabels(dataset=\"emotion_twitter_v10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "735f9a2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>coverage</th>\n",
       "      <th>annotated_coverage</th>\n",
       "      <th>overlaps</th>\n",
       "      <th>conflicts</th>\n",
       "      <th>correct</th>\n",
       "      <th>incorrect</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sedih</th>\n",
       "      <td>{sadness}</td>\n",
       "      <td>0.002617</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.000544</td>\n",
       "      <td>0.000480</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.818182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>senang</th>\n",
       "      <td>{happy}</td>\n",
       "      <td>0.002554</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seneng</th>\n",
       "      <td>{happy}</td>\n",
       "      <td>0.001871</td>\n",
       "      <td>0.006818</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marah</th>\n",
       "      <td>{anger}</td>\n",
       "      <td>0.001669</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>takut</th>\n",
       "      <td>{fear}</td>\n",
       "      <td>0.005247</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.000430</td>\n",
       "      <td>0.000367</td>\n",
       "      <td>55</td>\n",
       "      <td>11</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>serem</th>\n",
       "      <td>{fear}</td>\n",
       "      <td>0.001264</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sayang</th>\n",
       "      <td>{love}</td>\n",
       "      <td>0.003805</td>\n",
       "      <td>0.097727</td>\n",
       "      <td>0.000721</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>55</td>\n",
       "      <td>31</td>\n",
       "      <td>0.639535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cinta</th>\n",
       "      <td>{love}</td>\n",
       "      <td>0.002857</td>\n",
       "      <td>0.071591</td>\n",
       "      <td>0.000493</td>\n",
       "      <td>0.000265</td>\n",
       "      <td>48</td>\n",
       "      <td>15</td>\n",
       "      <td>0.761905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>astagfirullah</th>\n",
       "      <td>{anger}</td>\n",
       "      <td>0.000632</td>\n",
       "      <td>0.001136</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alhamdulillah</th>\n",
       "      <td>{happy}</td>\n",
       "      <td>0.004741</td>\n",
       "      <td>0.019318</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>0.823529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>khawatir</th>\n",
       "      <td>{fear}</td>\n",
       "      <td>0.000683</td>\n",
       "      <td>0.004545</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kasihan</th>\n",
       "      <td>{sadness}</td>\n",
       "      <td>0.000582</td>\n",
       "      <td>0.003409</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kesel</th>\n",
       "      <td>{anger}</td>\n",
       "      <td>0.001429</td>\n",
       "      <td>0.004545</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anjing</th>\n",
       "      <td>{anger}</td>\n",
       "      <td>0.001669</td>\n",
       "      <td>0.005682</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kontol</th>\n",
       "      <td>{anger}</td>\n",
       "      <td>0.000834</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>{love}</td>\n",
       "      <td>0.003944</td>\n",
       "      <td>0.010227</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>happy</th>\n",
       "      <td>{happy}</td>\n",
       "      <td>0.004640</td>\n",
       "      <td>0.013636</td>\n",
       "      <td>0.000341</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sad</th>\n",
       "      <td>{sadness}</td>\n",
       "      <td>0.000594</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meninggal</th>\n",
       "      <td>{sadness}</td>\n",
       "      <td>0.000544</td>\n",
       "      <td>0.002273</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bucin</th>\n",
       "      <td>{love}</td>\n",
       "      <td>0.000367</td>\n",
       "      <td>0.001136</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bahagia</th>\n",
       "      <td>{happy}</td>\n",
       "      <td>0.001820</td>\n",
       "      <td>0.015909</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Innalillahi</th>\n",
       "      <td>{sadness}</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>astaga</th>\n",
       "      <td>{sadness}</td>\n",
       "      <td>0.000759</td>\n",
       "      <td>0.001136</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>babi</th>\n",
       "      <td>{anger}</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.001136</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ngentot</th>\n",
       "      <td>{anger}</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>0.001136</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>*akit hat*</th>\n",
       "      <td>{sadness}</td>\n",
       "      <td>0.000860</td>\n",
       "      <td>0.007955</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>*atah hat*</th>\n",
       "      <td>{sadness}</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>0.002273</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total</th>\n",
       "      <td>{love, anger, happy, fear, sadness}</td>\n",
       "      <td>0.044918</td>\n",
       "      <td>0.330682</td>\n",
       "      <td>0.002491</td>\n",
       "      <td>0.001707</td>\n",
       "      <td>249</td>\n",
       "      <td>98</td>\n",
       "      <td>0.717579</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             label  coverage  \\\n",
       "sedih                                    {sadness}  0.002617   \n",
       "senang                                     {happy}  0.002554   \n",
       "seneng                                     {happy}  0.001871   \n",
       "marah                                      {anger}  0.001669   \n",
       "takut                                       {fear}  0.005247   \n",
       "serem                                       {fear}  0.001264   \n",
       "sayang                                      {love}  0.003805   \n",
       "cinta                                       {love}  0.002857   \n",
       "astagfirullah                              {anger}  0.000632   \n",
       "alhamdulillah                              {happy}  0.004741   \n",
       "khawatir                                    {fear}  0.000683   \n",
       "kasihan                                  {sadness}  0.000582   \n",
       "kesel                                      {anger}  0.001429   \n",
       "anjing                                     {anger}  0.001669   \n",
       "kontol                                     {anger}  0.000834   \n",
       "love                                        {love}  0.003944   \n",
       "happy                                      {happy}  0.004640   \n",
       "sad                                      {sadness}  0.000594   \n",
       "meninggal                                {sadness}  0.000544   \n",
       "bucin                                       {love}  0.000367   \n",
       "bahagia                                    {happy}  0.001820   \n",
       "Innalillahi                              {sadness}  0.000076   \n",
       "astaga                                   {sadness}  0.000759   \n",
       "babi                                       {anger}  0.000961   \n",
       "ngentot                                    {anger}  0.000417   \n",
       "*akit hat*                               {sadness}  0.000860   \n",
       "*atah hat*                               {sadness}  0.000190   \n",
       "total          {love, anger, happy, fear, sadness}  0.044918   \n",
       "\n",
       "               annotated_coverage  overlaps  conflicts  correct  incorrect  \\\n",
       "sedih                    0.012500  0.000544   0.000480        9          2   \n",
       "senang                   0.000000  0.000177   0.000139        0          0   \n",
       "seneng                   0.006818  0.000303   0.000253        2          4   \n",
       "marah                    0.012500  0.000303   0.000215        8          3   \n",
       "takut                    0.075000  0.000430   0.000367       55         11   \n",
       "serem                    0.022727  0.000126   0.000088       18          2   \n",
       "sayang                   0.097727  0.000721   0.000442       55         31   \n",
       "cinta                    0.071591  0.000493   0.000265       48         15   \n",
       "astagfirullah            0.001136  0.000051   0.000025        1          0   \n",
       "alhamdulillah            0.019318  0.000253   0.000139       14          3   \n",
       "khawatir                 0.004545  0.000051   0.000025        3          1   \n",
       "kasihan                  0.003409  0.000013   0.000000        1          2   \n",
       "kesel                    0.004545  0.000164   0.000088        2          2   \n",
       "anjing                   0.005682  0.000228   0.000177        2          3   \n",
       "kontol                   0.000000  0.000038   0.000000        0          0   \n",
       "love                     0.010227  0.000253   0.000139        9          0   \n",
       "happy                    0.013636  0.000341   0.000253        7          5   \n",
       "sad                      0.000000  0.000025   0.000025        0          0   \n",
       "meninggal                0.002273  0.000051   0.000051        1          1   \n",
       "bucin                    0.001136  0.000013   0.000000        0          1   \n",
       "bahagia                  0.015909  0.000303   0.000215        6          8   \n",
       "Innalillahi              0.000000  0.000000   0.000000        0          0   \n",
       "astaga                   0.001136  0.000025   0.000025        0          1   \n",
       "babi                     0.001136  0.000051   0.000013        1          0   \n",
       "ngentot                  0.001136  0.000013   0.000000        1          0   \n",
       "*akit hat*               0.007955  0.000126   0.000076        4          3   \n",
       "*atah hat*               0.002273  0.000101   0.000076        2          0   \n",
       "total                    0.330682  0.002491   0.001707      249         98   \n",
       "\n",
       "               precision  \n",
       "sedih           0.818182  \n",
       "senang               NaN  \n",
       "seneng          0.333333  \n",
       "marah           0.727273  \n",
       "takut           0.833333  \n",
       "serem           0.900000  \n",
       "sayang          0.639535  \n",
       "cinta           0.761905  \n",
       "astagfirullah   1.000000  \n",
       "alhamdulillah   0.823529  \n",
       "khawatir        0.750000  \n",
       "kasihan         0.333333  \n",
       "kesel           0.500000  \n",
       "anjing          0.400000  \n",
       "kontol               NaN  \n",
       "love            1.000000  \n",
       "happy           0.583333  \n",
       "sad                  NaN  \n",
       "meninggal       0.500000  \n",
       "bucin           0.000000  \n",
       "bahagia         0.428571  \n",
       "Innalillahi          NaN  \n",
       "astaga          0.000000  \n",
       "babi            1.000000  \n",
       "ngentot         1.000000  \n",
       "*akit hat*      0.571429  \n",
       "*atah hat*      1.000000  \n",
       "total           0.717579  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weak_labels.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bdb7929f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 273.26epoch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       happy       0.67      0.65      0.66        43\n",
      "       anger       0.67      0.43      0.52        28\n",
      "     sadness       0.65      0.28      0.39        39\n",
      "        fear       0.85      0.89      0.87        80\n",
      "        love       0.70      0.90      0.79       101\n",
      "\n",
      "    accuracy                           0.73       291\n",
      "   macro avg       0.71      0.63      0.65       291\n",
      "weighted avg       0.72      0.73      0.71       291\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from rubrix.labeling.text_classification import Snorkel\n",
    "\n",
    "label_model = Snorkel(weak_labels)\n",
    "\n",
    "label_model.fit(lr=0.002, n_epochs=10, progress_bar=True)\n",
    "print(label_model.score(output_str=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4f7896cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<rubrix.labeling.text_classification.weak_labels.WeakLabels at 0x20c2ba02170>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weak_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44f225bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Soal jln Jatibaru,polisi tdk bs GERTAK gubernu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Sesama cewe lho (kayaknya), harusnya bisa lebi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Kepingin gudeg mbarek Bu hj. Amad Foto dari go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Jln Jatibaru,bagian dari wilayah Tn Abang.Peng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Sharing pengalaman aja, kemarin jam 18.00 bata...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4396</th>\n",
       "      <td>4</td>\n",
       "      <td>Tahukah kamu, bahwa saat itu papa memejamkan m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4397</th>\n",
       "      <td>3</td>\n",
       "      <td>Sulitnya menetapkan Calon Wapresnya Jokowi di ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4398</th>\n",
       "      <td>1</td>\n",
       "      <td>5. masa depannya nggak jelas. lha iya, gimana ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4399</th>\n",
       "      <td>0</td>\n",
       "      <td>[USERNAME] dulu beneran ada mahasiswa Teknik U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4400</th>\n",
       "      <td>2</td>\n",
       "      <td>Ya Allah, hanya Engkau yang mengetahui rasa sa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4401 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                              tweet\n",
       "0         1  Soal jln Jatibaru,polisi tdk bs GERTAK gubernu...\n",
       "1         1  Sesama cewe lho (kayaknya), harusnya bisa lebi...\n",
       "2         0  Kepingin gudeg mbarek Bu hj. Amad Foto dari go...\n",
       "3         1  Jln Jatibaru,bagian dari wilayah Tn Abang.Peng...\n",
       "4         0  Sharing pengalaman aja, kemarin jam 18.00 bata...\n",
       "...     ...                                                ...\n",
       "4396      4  Tahukah kamu, bahwa saat itu papa memejamkan m...\n",
       "4397      3  Sulitnya menetapkan Calon Wapresnya Jokowi di ...\n",
       "4398      1  5. masa depannya nggak jelas. lha iya, gimana ...\n",
       "4399      0  [USERNAME] dulu beneran ada mahasiswa Teknik U...\n",
       "4400      2  Ya Allah, hanya Engkau yang mengetahui rasa sa...\n",
       "\n",
       "[4401 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_labelled['label'] = data_labelled['label'].replace(['happy', 'anger', 'sadness', 'fear', 'love'],[0, 1, 2, 3, 4]  )\n",
    "data_labelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b439892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "309c5f45",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\pradi\\emotion classification thesis.ipynb Cell 38\u001b[0m in \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pradi/emotion%20classification%20thesis.ipynb#X52sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mwith\u001b[39;00m pd\u001b[39m.\u001b[39moption_context(\u001b[39m'\u001b[39m\u001b[39mdisplay.max_colwidth\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/pradi/emotion%20classification%20thesis.ipynb#X52sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     display(pd\u001b[39m.\u001b[39mDataFrame({\u001b[39m\"\u001b[39m\u001b[39mtweet\u001b[39m\u001b[39m\"\u001b[39m: x_train, \u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m: y_train}))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(pd.DataFrame({\"tweet\": x_train, \"label\": y_train}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "baaf9219",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data_labelled['tweet']\n",
    "y = data_labelled['label']\n",
    "\n",
    "X_train_normal, X_test_normal, y_train_normal, y_test_normal = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6d75fa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_X_train = pd.concat([snorkel_data_20['tweet'], X_train_normal])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0ff831dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_y_train = pd.concat([snorkel_data_20['label'], y_train_normal])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b6094e",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'base' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "combined_y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cdb37515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       2\n",
       "1       0\n",
       "2       4\n",
       "3       4\n",
       "4       0\n",
       "       ..\n",
       "3444    0\n",
       "466     1\n",
       "3092    2\n",
       "3772    1\n",
       "860     2\n",
       "Name: label, Length: 6820, dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dd09e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_X_train.to_csv('D:\\Downloads\\Combined_X_train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2108bcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_y_train.to_csv('D:\\Downloads\\Combined_y_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6e36aaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4401"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_labelled['tweet'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2c5bba78",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = label_model.predict()\n",
    "label2int = {'happy': 0, 'anger': 1, 'sadness': 2, 'fear': 3, 'love': 4}\n",
    "x_train = [rec.text for rec in records]\n",
    "y_train = [label2int[rec.prediction[0][0]] for rec in records]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de867757",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2int = {'happy': 0, 'anger': 1, 'sadness': 2, 'fear': 3, 'love': 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "db3a4436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wihh ikutan happy Congrats sista</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Di marah marahin terus sama bagol padahal sepanjang pertandingan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jangan sedih cantik</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Masih syukur UMR Saya dibawah umur Cuman Dan kasihan lagi yang dibawah saya ORANG KAYA ADALAH ORANG SELALU MERASA CUKUP ORANG MISKIN ADALAH ORANG YANG SELALU MERASA KURANG INGAT SAAT NANTI NAFAS STOP DARI BADANMU APA YANG AKAN KAMU PERBUAT DENGAN HARTAMU INGAT MATI</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Happy easter</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3255</th>\n",
       "      <td>Orang takut mah ngumpett ini takut malah pengen bj Takut aja terus selagi nonton dah wkwkk</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3256</th>\n",
       "      <td>Patah hati terbesar personil member bukan sebatas memutuskan keluar dari team tp kehilangan member dgn cara menyedihkan dan LBH miris lagi saat dia membutuhkan sosok disamping dia ngerasa sendiri sampai akhir hayatnya ahhh andai waktu bs diputar mungkin cerita akan beda</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3257</th>\n",
       "      <td>Gacocok jadi influencer baru difollo banyak cowo aja bawaannya malah takut</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>Puasa dibikin kesel emg ada aja da ampun</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>Salwa kenapa akhir ini nangis mulu itu skinkernya mon maap sayang</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3260 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                               tweet  \\\n",
       "0                                                                                                                                                                                                                                                   Wihh ikutan happy Congrats sista   \n",
       "1                                                                                                                                                                                                                   Di marah marahin terus sama bagol padahal sepanjang pertandingan   \n",
       "2                                                                                                                                                                                                                                                                Jangan sedih cantik   \n",
       "3          Masih syukur UMR Saya dibawah umur Cuman Dan kasihan lagi yang dibawah saya ORANG KAYA ADALAH ORANG SELALU MERASA CUKUP ORANG MISKIN ADALAH ORANG YANG SELALU MERASA KURANG INGAT SAAT NANTI NAFAS STOP DARI BADANMU APA YANG AKAN KAMU PERBUAT DENGAN HARTAMU INGAT MATI   \n",
       "4                                                                                                                                                                                                                                                                       Happy easter   \n",
       "...                                                                                                                                                                                                                                                                              ...   \n",
       "3255                                                                                                                                                                                      Orang takut mah ngumpett ini takut malah pengen bj Takut aja terus selagi nonton dah wkwkk   \n",
       "3256  Patah hati terbesar personil member bukan sebatas memutuskan keluar dari team tp kehilangan member dgn cara menyedihkan dan LBH miris lagi saat dia membutuhkan sosok disamping dia ngerasa sendiri sampai akhir hayatnya ahhh andai waktu bs diputar mungkin cerita akan beda   \n",
       "3257                                                                                                                                                                                                      Gacocok jadi influencer baru difollo banyak cowo aja bawaannya malah takut   \n",
       "3258                                                                                                                                                                                                                                        Puasa dibikin kesel emg ada aja da ampun   \n",
       "3259                                                                                                                                                                                                               Salwa kenapa akhir ini nangis mulu itu skinkernya mon maap sayang   \n",
       "\n",
       "      label  \n",
       "0         0  \n",
       "1         1  \n",
       "2         2  \n",
       "3         2  \n",
       "4         0  \n",
       "...     ...  \n",
       "3255      3  \n",
       "3256      2  \n",
       "3257      3  \n",
       "3258      1  \n",
       "3259      4  \n",
       "\n",
       "[3260 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(pd.DataFrame({\"tweet\": x_train, \"label\": y_train}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "56cd8656",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({\"tweet\": x_train, \"label\": y_train})\n",
    "df.to_csv(\"D:\\Downloads\\Snorkel_dataset_with_20_test.csv\", encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2cdfe22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>org org knpa pada jahat sama gw hari ini astaga</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Happy Birthday</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ShaniJKT aamiin semangat sayangku bimbingan ad...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Rykarl bucin sangat smp suruh Fir minta maaf k...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>funsized Happy birthday</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>995</td>\n",
       "      <td>Sekarang liat yang lucu dikit aja udah mulai k...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>996</td>\n",
       "      <td>The boss baby How to train your dragon The emo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>997</td>\n",
       "      <td>HASIL PENGELUARAN HUAHIN Hari Ini SABTU April ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>998</td>\n",
       "      <td>kalau boleh soalnya takut kekampus sendiri   a...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>999</td>\n",
       "      <td>love you</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                              tweet  label\n",
       "0             0    org org knpa pada jahat sama gw hari ini astaga      2\n",
       "1             1                                     Happy Birthday      0\n",
       "2             2  ShaniJKT aamiin semangat sayangku bimbingan ad...      4\n",
       "3             3  Rykarl bucin sangat smp suruh Fir minta maaf k...      4\n",
       "4             4                            funsized Happy birthday      0\n",
       "..          ...                                                ...    ...\n",
       "995         995  Sekarang liat yang lucu dikit aja udah mulai k...      0\n",
       "996         996  The boss baby How to train your dragon The emo...      0\n",
       "997         997  HASIL PENGELUARAN HUAHIN Hari Ini SABTU April ...      1\n",
       "998         998  kalau boleh soalnya takut kekampus sendiri   a...      3\n",
       "999         999                                           love you      4\n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snorkel_data = pd.read_csv(\"D:\\Downloads\\Snorkel_dataset_v7.csv\")\n",
    "# snorkel_data['label'] = snorkel_data['label'].replace([0, 1, 2, 3, 4], ['happy', 'anger', 'sadness', 'fear', 'love'] )\n",
    "snorkel_data.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9ec1639a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>org org knpa pada jahat sama gw hari ini astaga</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Happy Birthday</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ShaniJKT aamiin semangat sayangku bimbingan ad...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Rykarl bucin sangat smp suruh Fir minta maaf k...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>funsized Happy birthday</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3295</th>\n",
       "      <td>3295</td>\n",
       "      <td>Happy birthday boss</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3296</th>\n",
       "      <td>3296</td>\n",
       "      <td>Yeayy Alhamdulillah lazada pecah telor today s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3297</th>\n",
       "      <td>3297</td>\n",
       "      <td>kaget lebih ke takut si</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3298</th>\n",
       "      <td>3298</td>\n",
       "      <td>Astaga lupa</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3299</th>\n",
       "      <td>3299</td>\n",
       "      <td>Dengan senang hati kami informasikan Untuk jam...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3300 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                              tweet  label\n",
       "0              0    org org knpa pada jahat sama gw hari ini astaga      2\n",
       "1              1                                     Happy Birthday      0\n",
       "2              2  ShaniJKT aamiin semangat sayangku bimbingan ad...      4\n",
       "3              3  Rykarl bucin sangat smp suruh Fir minta maaf k...      4\n",
       "4              4                            funsized Happy birthday      0\n",
       "...          ...                                                ...    ...\n",
       "3295        3295                                Happy birthday boss      0\n",
       "3296        3296  Yeayy Alhamdulillah lazada pecah telor today s...      0\n",
       "3297        3297                            kaget lebih ke takut si      3\n",
       "3298        3298                                        Astaga lupa      2\n",
       "3299        3299  Dengan senang hati kami informasikan Untuk jam...      0\n",
       "\n",
       "[3300 rows x 3 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snorkel_data_20 = pd.read_csv(\"D:\\Downloads\\Snorkel_dataset_with_20_test.csv\")\n",
    "# snorkel_data['label'] = snorkel_data['label'].replace([0, 1, 2, 3, 4], ['happy', 'anger', 'sadness', 'fear', 'love'] )\n",
    "snorkel_data_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e21f811f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>org org knpa pada jahat sama gw hari ini astaga</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Happy Birthday</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ShaniJKT aamiin semangat sayangku bimbingan ad...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rykarl bucin sangat smp suruh Fir minta maaf k...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>funsized Happy birthday</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3295</th>\n",
       "      <td>Happy birthday boss</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3296</th>\n",
       "      <td>Yeayy Alhamdulillah lazada pecah telor today s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3297</th>\n",
       "      <td>kaget lebih ke takut si</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3298</th>\n",
       "      <td>Astaga lupa</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3299</th>\n",
       "      <td>Dengan senang hati kami informasikan Untuk jam...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3300 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet  label\n",
       "0       org org knpa pada jahat sama gw hari ini astaga      2\n",
       "1                                        Happy Birthday      0\n",
       "2     ShaniJKT aamiin semangat sayangku bimbingan ad...      4\n",
       "3     Rykarl bucin sangat smp suruh Fir minta maaf k...      4\n",
       "4                               funsized Happy birthday      0\n",
       "...                                                 ...    ...\n",
       "3295                                Happy birthday boss      0\n",
       "3296  Yeayy Alhamdulillah lazada pecah telor today s...      0\n",
       "3297                            kaget lebih ke takut si      3\n",
       "3298                                        Astaga lupa      2\n",
       "3299  Dengan senang hati kami informasikan Untuk jam...      0\n",
       "\n",
       "[3300 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snorkel_data.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9d07e719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3300"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snorkel_data_20['label'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6b42c13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    942\n",
       "4    553\n",
       "1    437\n",
       "3    371\n",
       "2    337\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "train_data_snorkel, test_data_snorkel = train_test_split(snorkel_data, test_size=0.2, random_state=42)\n",
    "train_data_snorkel['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "686db4ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    226\n",
       "2    213\n",
       "0    195\n",
       "4    124\n",
       "3    122\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "163aa3bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vect&#x27;, CountVectorizer()), (&#x27;clf&#x27;, SVC(kernel=&#x27;linear&#x27;))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vect&#x27;, CountVectorizer()), (&#x27;clf&#x27;, SVC(kernel=&#x27;linear&#x27;))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer()), ('clf', SVC(kernel='linear'))])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, TfidfVectorizer\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import svm\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\")\n",
    "# model = TFAutoModel.from_pretrained(\"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\", from_pt=True)\n",
    "\n",
    "classifier = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    # ('tfidf', TfidfVectorizer(tokenizer=tokenizer.tokenize)),\n",
    "    ('clf', svm.SVC(kernel='linear'))\n",
    "])\n",
    "\n",
    "classifier.fit(X=combined_X_train, y=combined_y_train,)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70c5c85c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "acc9a973",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = weak_labels.records(has_annotation=True)\n",
    "\n",
    "\n",
    "X_test = [rec.text for rec in test_ds]\n",
    "y_test = [label2int[rec.annotation] for rec in test_ds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b1ea5993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.6140749148694665\n"
     ]
    }
   ],
   "source": [
    "accuracy = classifier.score(\n",
    "    X=X_test_normal,\n",
    "    y=y_test_normal,\n",
    ")\n",
    "\n",
    "print(f\"Test accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ab3055e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       happy       0.57      0.58      0.58       214\n",
      "       anger       0.62      0.67      0.65       229\n",
      "     sadness       0.49      0.48      0.48       200\n",
      "        fear       0.74      0.62      0.68       119\n",
      "        love       0.76      0.78      0.77       119\n",
      "\n",
      "    accuracy                           0.61       881\n",
      "   macro avg       0.64      0.63      0.63       881\n",
      "weighted avg       0.62      0.61      0.61       881\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# get predictions for the test set\n",
    "predicted = classifier.predict(X_test_normal)\n",
    "\n",
    "print(metrics.classification_report(y_test_normal, predicted, target_names=label2int.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "29a68a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:745: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_scores = torch.bmm(query_layer, key_layer.transpose(-1, -2)) / torch.tensor(\n",
      "C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:829: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  score += c2p_att / torch.tensor(scale, dtype=c2p_att.dtype)\n",
      "C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:852: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  score += p2c_att / torch.tensor(scale, dtype=p2c_att.dtype)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [25], line 52\u001b[0m\n\u001b[0;32m     44\u001b[0m label_encoder\u001b[38;5;241m.\u001b[39mfit(train_labels)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# train_labels_encoded = label_encoder.transform(train_labels)\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# test_labels_encoded = label_encoder.transform(test_labels)\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# train_probs = classifier(train_texts, candidate_labels)[\"scores\"]\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# test_probs = classifier(test_text, candidate_labels)[\"scores\"]\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m train_probs \u001b[38;5;241m=\u001b[39m \u001b[43mclassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcandidate_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscores\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     53\u001b[0m test_probs \u001b[38;5;241m=\u001b[39m classifier(test_texts, candidate_labels)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     55\u001b[0m svm \u001b[38;5;241m=\u001b[39m SVC(kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_name = \"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\", model=model, tokenizer=tokenizer)\n",
    "candidate_labels = [\"love\", \"happy\", \"sadness\", \"anger\", \"fear\"]\n",
    "\n",
    "# def classify_text(text):\n",
    "#     result = classifier(text, candidate_labels)\n",
    "#     predicted_label = result[\"labels\"][0]\n",
    "#     score = result[\"scores\"][0]\n",
    "#     return predicted_label, score\n",
    "\n",
    "# def extract_probs(text):\n",
    "#     result = classifier(text, candidate_labels)\n",
    "#     probs = result[\"scores\"]\n",
    "#     return probs\n",
    "\n",
    "train_data_snorkel, test_data_snorkel = train_test_split(snorkel_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# training_data_saputri=pd.read_csv(\"D:\\Downloads\\Twitter_Emotion_Dataset_with_label.csv\")\n",
    "\n",
    "# snorkel_data[[\"predicted_label\", \"score\"]] = snorkel_data[\"tweet\"].apply(classify_text).apply(pd.Series)\n",
    "# snorkel_data.to_csv(\"classified_snorkel_data_v2.csv\", index=False)\n",
    "\n",
    "\n",
    "# Split the data into training and testing datasets\n",
    "train_texts = train_data_snorkel[\"tweet\"].tolist()\n",
    "train_labels = train_data_snorkel[\"label\"].tolist()\n",
    "test_texts = test_data_snorkel[\"tweet\"].tolist()\n",
    "test_labels = test_data_snorkel[\"label\"].tolist()\n",
    "\n",
    "# label_encoder = LabelEncoder()\n",
    "# label_encoder.fit(train_labels)\n",
    "\n",
    "# train_labels_encoded = label_encoder.transform(train_labels)\n",
    "# test_labels_encoded = label_encoder.transform(test_labels)\n",
    "\n",
    "# train_probs = classifier(train_texts, candidate_labels)[\"scores\"]\n",
    "# test_probs = classifier(test_text, candidate_labels)[\"scores\"]\n",
    "\n",
    "train_probs = classifier(train_texts, candidate_labels)[\"scores\"]\n",
    "test_probs = classifier(test_texts, candidate_labels)[\"scores\"]\n",
    "\n",
    "svm = SVC(kernel='linear')\n",
    "svm.fit(train_probs, train_labels)\n",
    "\n",
    "predicted_labels = svm.predict(test_probs)\n",
    "\n",
    "# predicted_labels_encoded = svm.predict(test_probs)\n",
    "\n",
    "# predicted_labels = label_encoder.inverse_transform(predicted_labels_encoded)\n",
    "\n",
    "accuracy = accuracy_score(test_labels, predicted_labels)\n",
    "f1 = f1_score(test_labels, predicted_labels, average='weighted')\n",
    "recall = recall_score(test_labels, predicted_labels, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "\n",
    "# svm = SVC(kernel='linear')\n",
    "\n",
    "# svm.fit(train_features, train_labels)\n",
    "\n",
    "# predicted_labels = svm.predict(test_features)\n",
    "\n",
    "# accuracy = accuracy_score(test_labels, predicted_labels)\n",
    "# f1 = f1_score(test_labels, predicted_labels, average='weighted')\n",
    "# recall = recall_score(test_labels, predicted_labels, average='weighted')\n",
    "\n",
    "# print(f\"Accuracy: {accuracy}\")\n",
    "# print(f\"F1 Score: {f1}\")\n",
    "# print(f\"Recall: {recall}\")\n",
    "\n",
    "# probs_list = snorkel_data[\"tweet\"].apply(extract_probs).tolist()\n",
    "# probs_df = pd.DataFrame(probs_list, columns=candidate_labels)\n",
    "\n",
    "# train_data_snorkel = pd.concat([train_data_snorkel.reset_index(drop=True), probs_df], axis=1)\n",
    "\n",
    "# train_data_snorkel.fillna(train_data_snorkel.mean(), inplace=True)\n",
    "\n",
    "# train_features = train_data_snorkel[candidate_labels].values\n",
    "# train_labels = train_data_snorkel[\"label\"].tolist()\n",
    "\n",
    "# test_data_snorkel = pd.concat([test_data_snorkel.reset_index(drop=True), probs_df], axis=1)\n",
    "\n",
    "# test_data_snorkel.fillna(test_data_snorkel.mean(), inplace=True)\n",
    "\n",
    "\n",
    "# test_features = test_data_snorkel[candidate_labels].values\n",
    "# test_labels = test_data_snorkel[\"label\"].tolist()\n",
    "\n",
    "\n",
    "# features = snorkel_data[candidate_labels].values\n",
    "# labels = snorkel_data[\"label\"].tolist()\n",
    "\n",
    "\n",
    "# classifier = pipeline(\"zero-shot-classification\", model=model, tokenizer=tokenizer)\n",
    "# sequence_to_classify = classifier(snorkel_data['label'])\n",
    "# candidate_labels = [\"love\", \"happy\", \"sadness\", \"anger\",\"fear\"]\n",
    "# output = classifier(candidate_labels, multi_label=False)\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85413667",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [21], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m test_texts \u001b[38;5;241m=\u001b[39m test_data_snorkel[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtweet\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     24\u001b[0m test_labels \u001b[38;5;241m=\u001b[39m test_data_snorkel[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m---> 26\u001b[0m train_probs \u001b[38;5;241m=\u001b[39m \u001b[43mclassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcandidate_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscores\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     27\u001b[0m test_probs \u001b[38;5;241m=\u001b[39m classifier(test_texts, candidate_labels)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Convert string labels to integer labels\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "model_name = \"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\", model=model, tokenizer=tokenizer)\n",
    "candidate_labels = [\"love\", \"happy\", \"sadness\", \"anger\",\"fear\"]\n",
    "\n",
    "train_data_snorkel, test_data_snorkel = train_test_split(snorkel_data, test_size=0.2, random_state=42)\n",
    "# this thing is cursing me, i'll try the other thing later refer to the training data code above\n",
    "\n",
    "# Convert the dictionary to a pandas DataFrame\n",
    "train_data_snorkel = pd.DataFrame.from_dict(snorkel_data)\n",
    "\n",
    "def classify_text(text):\n",
    "    result = classifier(text, candidate_labels)\n",
    "    predicted_label = result[\"labels\"][0]\n",
    "    score = result[\"scores\"][0]\n",
    "    return predicted_label, score\n",
    "\n",
    "# train_texts = train_data_snorkel[\"tweet\"].tolist()\n",
    "# train_labels = train_data_snorkel[\"label\"].tolist()\n",
    "# test_texts = test_data_snorkel[\"tweet\"].tolist()\n",
    "# test_labels = test_data_snorkel[\"label\"].tolist()\n",
    "\n",
    "# train_probs = classifier(train_texts, candidate_labels)[\"scores\"]\n",
    "# test_probs = classifier(test_texts, candidate_labels)[\"scores\"]\n",
    "\n",
    "# Convert string labels to integer labels\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(train_labels)\n",
    "train_labels_encoded = label_encoder.transform(train_labels)\n",
    "\n",
    "svm = SVC(kernel='linear')\n",
    "svm.fit(train_probs, train_labels_encoded)\n",
    "\n",
    "# Convert string test labels to integer labels\n",
    "test_labels_encoded = label_encoder.transform(test_labels)\n",
    "\n",
    "predicted_labels = svm.predict(test_probs)\n",
    "\n",
    "accuracy = accuracy_score(test_labels_encoded, predicted_labels)\n",
    "f1 = f1_score(test_labels_encoded, predicted_labels, average='weighted')\n",
    "recall = recall_score(test_labels_encoded, predicted_labels, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Recall: {recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1ab6f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:745: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_scores = torch.bmm(query_layer, key_layer.transpose(-1, -2)) / torch.tensor(\n",
      "C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:829: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  score += c2p_att / torch.tensor(scale, dtype=c2p_att.dtype)\n",
      "C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:852: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  score += p2c_att / torch.tensor(scale, dtype=p2c_att.dtype)\n",
      "C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:745: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_scores = torch.bmm(query_layer, key_layer.transpose(-1, -2)) / torch.tensor(\n",
      "C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:829: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  score += c2p_att / torch.tensor(scale, dtype=c2p_att.dtype)\n",
      "C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:852: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  score += p2c_att / torch.tensor(scale, dtype=p2c_att.dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       happy       0.00      0.00      0.00       106\n",
      "       anger       0.00      0.00      0.00       108\n",
      "     sadness       0.37      1.00      0.53       241\n",
      "        fear       0.00      0.00      0.00       122\n",
      "        love       0.00      0.00      0.00        83\n",
      "\n",
      "    accuracy                           0.37       660\n",
      "   macro avg       0.07      0.20      0.11       660\n",
      "weighted avg       0.13      0.37      0.20       660\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score\n",
    "\n",
    "train_data, test_data = train_test_split(snorkel_data, test_size=0.2, random_state=42)\n",
    "\n",
    "model_name = \"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "classifier = pipeline(\"zero-shot-classification\", model=model, tokenizer=tokenizer)\n",
    "candidate_labels = ['happy', 'anger', 'sadness', 'fear', 'love']\n",
    "\n",
    "def extract_probs(text):\n",
    "    result = classifier(text, candidate_labels)\n",
    "    probs = result[\"scores\"]\n",
    "    return probs\n",
    "\n",
    "train_data[candidate_labels] = train_data[\"tweet\"].apply(extract_probs).apply(pd.Series)\n",
    "\n",
    "train_features = train_data[candidate_labels].values\n",
    "train_labels = train_data[\"label\"].values\n",
    "\n",
    "svm = SVC(kernel='linear')\n",
    "\n",
    "svm.fit(train_features, train_labels)\n",
    "\n",
    "test_data[candidate_labels] = test_data[\"tweet\"].apply(extract_probs).apply(pd.Series)\n",
    "\n",
    "test_features = test_data[candidate_labels].values\n",
    "test_labels = test_data[\"label\"].values\n",
    "\n",
    "predicted_labels = svm.predict(test_features)\n",
    "\n",
    "accuracy = accuracy_score(test_labels, predicted_labels)\n",
    "f1 = f1_score(test_labels, predicted_labels, average='weighted')\n",
    "recall = recall_score(test_labels, predicted_labels, average='weighted')\n",
    "print(classification_report(test_labels, predicted_labels, target_names=candidate_labels))\n",
    "\n",
    "# print(f\"Accuracy: {accuracy}\")\n",
    "# print(f\"F1 Score: {f1}\")\n",
    "# print(f\"Recall: {recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3edd801",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:745: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_scores = torch.bmm(query_layer, key_layer.transpose(-1, -2)) / torch.tensor(\n",
      "C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:829: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  score += c2p_att / torch.tensor(scale, dtype=c2p_att.dtype)\n",
      "C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:852: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  score += p2c_att / torch.tensor(scale, dtype=p2c_att.dtype)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas' has no attribute 'Dataframe'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [12], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m probs\n\u001b[0;32m     21\u001b[0m probs_list \u001b[38;5;241m=\u001b[39m snorkel_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtweet\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(extract_probs)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m---> 22\u001b[0m probs_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataframe\u001b[49m(probs_list, columns\u001b[38;5;241m=\u001b[39mcandidate_labels)\n\u001b[0;32m     24\u001b[0m snorkel_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([snorkel_data, probs_df], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     26\u001b[0m features \u001b[38;5;241m=\u001b[39m snorkel_data[candidate_labels]\u001b[38;5;241m.\u001b[39mvalues\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\__init__.py:264\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparseArray \u001b[38;5;28;01mas\u001b[39;00m _SparseArray\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _SparseArray\n\u001b[1;32m--> 264\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpandas\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'Dataframe'"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score\n",
    "\n",
    "model_name = \"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\", model=model, tokenizer=tokenizer)\n",
    "candidate_labels = [\"love\", \"happy\", \"sadness\", \"anger\", \"fear\"]\n",
    "\n",
    "def extract_probs(text):\n",
    "    result = classifier(text, candidate_labels)\n",
    "    probs = result[\"scores\"]\n",
    "    return probs\n",
    "\n",
    "probs_list = snorkel_data[\"tweet\"].apply(extract_probs).tolist()\n",
    "probs_df = pd.Dataframe(probs_list, columns=candidate_labels)\n",
    "\n",
    "snorkel_data = pd.concat([snorkel_data, probs_df], axis=1)\n",
    "\n",
    "features = snorkel_data[candidate_labels].values\n",
    "labels = snorkel_data[\"label\"].tolist()\n",
    "\n",
    "svm = SVC(kernel='linear')\n",
    "\n",
    "svm.fit(features, labels)\n",
    "\n",
    "predicted_labels = svm.predict(features)\n",
    "\n",
    "print(classification_report(labels, predicted_labels, target_names=candidate_labels))\n",
    "\n",
    "# accuracy = accuracy_score(labels, predicted_labels)\n",
    "# f1 = f1_score(labels, predicted_labels, average='weighted')\n",
    "# recall = recall_score(labels, predicted_labels, average='weighted')\n",
    "\n",
    "# # print the performance metrics\n",
    "# print(f\"Accuracy: {accuracy}\")\n",
    "# print(f\"F1 Score: {f1}\")\n",
    "# print(f\"Recall: {recall}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7693df0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 529 entries, 0 to 528\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  529 non-null    int64 \n",
      " 1   tweet       529 non-null    object\n",
      " 2   label       529 non-null    object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 12.5+ KB\n"
     ]
    }
   ],
   "source": [
    "snorkel_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "341597d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "UnimplementedError",
     "evalue": "Graph execution error:\n\nDetected at node 'categorical_crossentropy/Cast' defined at (most recent call last):\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\traitlets\\config\\application.py\", line 978, in launch_instance\n      app.start()\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 1899, in _run_once\n      handle._run()\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2885, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2940, in _run_cell\n      return runner(coro)\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3139, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3318, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3378, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\pradi\\AppData\\Local\\Temp\\ipykernel_15120\\2668139211.py\", line 43, in <module>\n      model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test))\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 994, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1052, in compute_loss\n      return self.compiled_loss(\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 272, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 1977, in categorical_crossentropy\n      y_true = tf.cast(y_true, y_pred.dtype)\nNode: 'categorical_crossentropy/Cast'\nCast string to float is not supported\n\t [[{{node categorical_crossentropy/Cast}}]] [Op:__inference_train_function_18609]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [9], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mUnimplementedError\u001b[0m: Graph execution error:\n\nDetected at node 'categorical_crossentropy/Cast' defined at (most recent call last):\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\traitlets\\config\\application.py\", line 978, in launch_instance\n      app.start()\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 1899, in _run_once\n      handle._run()\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2885, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2940, in _run_cell\n      return runner(coro)\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3139, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3318, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3378, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\pradi\\AppData\\Local\\Temp\\ipykernel_15120\\2668139211.py\", line 43, in <module>\n      model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test))\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 994, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1052, in compute_loss\n      return self.compiled_loss(\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 272, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\pradi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 1977, in categorical_crossentropy\n      y_true = tf.cast(y_true, y_pred.dtype)\nNode: 'categorical_crossentropy/Cast'\nCast string to float is not supported\n\t [[{{node categorical_crossentropy/Cast}}]] [Op:__inference_train_function_18609]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Flatten\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"D:\\Downloads\\Twitter_Emotion_Dataset_with_label.csv\")\n",
    "\n",
    "# Define the maximum number of words to be used in the tokenizer\n",
    "max_words = 5000\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer(num_words=max_words, split=' ')\n",
    "tokenizer.fit_on_texts(df['tweet'].values)\n",
    "\n",
    "# Convert the text to sequences of integers\n",
    "X = tokenizer.texts_to_sequences(df['tweet'].values)\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "X = pad_sequences(X)\n",
    "\n",
    "# Define the labels\n",
    "y = df['label'].values\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Define the model architecture\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(max_words, 128),\n",
    "    tf.keras.layers.LSTM(128, dropout=0.2, recurrent_dropout=0.2),\n",
    "    tf.keras.layers.Dense(1, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a608448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# Load data from CSV file\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "# Define text and label columns\n",
    "text_col = 'text'\n",
    "label_col = 'emotion'\n",
    "\n",
    "# Convert text labels to numerical labels\n",
    "label_map = {label: i for i, label in enumerate(df[label_col].unique())}\n",
    "df[label_col] = df[label_col].map(label_map)\n",
    "\n",
    "# Tokenize text data\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(df[text_col])\n",
    "X = tokenizer.texts_to_sequences(df[text_col])\n",
    "X = pad_sequences(X, maxlen=50)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_size = int(0.8 * len(df))\n",
    "X_train, y_train = X[:train_size], df[label_col][:train_size]\n",
    "X_test, y_test = X[train_size:], df[label_col][train_size:]\n",
    "\n",
    "# Define LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(5000, 64))\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "65cca64b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ZeroShotClassificationPipeline' object has no attribute 'score'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m(\n\u001b[0;32m      2\u001b[0m X\u001b[38;5;241m=\u001b[39mx_test,\n\u001b[0;32m      3\u001b[0m y\u001b[38;5;241m=\u001b[39my_test,)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ZeroShotClassificationPipeline' object has no attribute 'score'"
     ]
    }
   ],
   "source": [
    "accuracy = classifier.score(\n",
    "X=x_test,\n",
    "y=y_test,)\n",
    "\n",
    "print(f\"Test accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e42bc8ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0    529\n",
       "tweet         529\n",
       "label         529\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snorkel_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b83dcfd9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DebertaV2ForSequenceClassification' object has no attribute 'predict_proba'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [38], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m     22\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m---> 24\u001b[0m probs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m(test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     27\u001b[0m svm_train \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_proba(train_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     28\u001b[0m svm_test \u001b[38;5;241m=\u001b[39m probs\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1207\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1205\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1206\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1207\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1208\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DebertaV2ForSequenceClassification' object has no attribute 'predict_proba'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "classified_snorkel_data_v2 = pd.read_csv(\"C:/Users/pradi/classified_snorkel_data_v2.csv\")\n",
    "\n",
    "# predicted_labels = []\n",
    "# true_labels = []\n",
    "# for data in classified_snorkel_data_v2: \n",
    "#     text = data[\"tweet\"]\n",
    "#     true_label = data[\"label\"][0]\n",
    "#     labels = data[\"label\"]\n",
    "\n",
    "# train_data = classified_snorkel_data_v2[:80]\n",
    "# test_data = classified_snorkel_data_v2[80:]\n",
    "\n",
    "train_data = snorkel_data[:80]\n",
    "test_data = snorkel_data[80:]\n",
    "\n",
    "model_name = \"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "probs = model.predict_proba(test_data['text'])\n",
    "\n",
    "\n",
    "svm_train = model.predict_proba(train_data['text'])\n",
    "svm_test = probs\n",
    "\n",
    "svm = SVC(kernel='linear')\n",
    "svm.fit(svm_train, train_data['label'])\n",
    "predicted_labels = svm.predict(svm_test)\n",
    "\n",
    "accuracy = accuracy_score(test_data['label'], predicted_labels)\n",
    "f1 = f1_score(test_data['label'], predicted_labels, average='macro')\n",
    "recall = recall_score(test_data['label'], predicted_labels, average='macro')\n",
    "\n",
    "\n",
    "\n",
    "# accuracy = accuracy_score(test_data['label'], test_data['predicted_label'])\n",
    "# f1 = f1_score(test_data['label'], test_data['predicted_label'], average='macro')\n",
    "# recall = recall_score(test_data['label'], test_data['predicted_label'], average='macro')\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d20b8229",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classified_snorkel_dataset_v2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mclassified_snorkel_dataset_v2\u001b[49m ({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtweet\u001b[39m\u001b[38;5;124m\"\u001b[39m: x_train, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredicted_label\u001b[39m\u001b[38;5;124m\"\u001b[39m: y_train})\n",
      "\u001b[1;31mNameError\u001b[0m: name 'classified_snorkel_dataset_v2' is not defined"
     ]
    }
   ],
   "source": [
    "classified_snorkel_dataset_v2 ({\"tweet\": x_train, \"predicted_label\": y_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31dfa7e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [14], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m metrics\n\u001b[1;32m----> 3\u001b[0m predicted \u001b[38;5;241m=\u001b[39m classifier\u001b[38;5;241m.\u001b[39mpredict(\u001b[43mx_test\u001b[49m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(metrics\u001b[38;5;241m.\u001b[39mclassification_report(y_test, predicted, target_names\u001b[38;5;241m=\u001b[39mlabel2int\u001b[38;5;241m.\u001b[39mkeys()))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_test' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "predicted = classifier.predict(x_test)\n",
    "print(metrics.classification_report(y_test, predicted, target_names=label2int.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4681d8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at StevenLimcorn/indonesian-roberta-base-emotion-classifier were not used when initializing TFRobertaModel: ['classifier']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFRobertaModel were not initialized from the model checkpoint at StevenLimcorn/indonesian-roberta-base-emotion-classifier and are newly initialized: ['roberta/pooler/dense/kernel:0', 'roberta/pooler/dense/bias:0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The number of classes has to be greater than one; got 1 class",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [8], line 45\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Train your SVM classifier\u001b[39;00m\n\u001b[0;32m     44\u001b[0m clf \u001b[38;5;241m=\u001b[39m SVC(kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m\"\u001b[39m, C\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m---> 45\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Evaluate your classifier\u001b[39;00m\n\u001b[0;32m     48\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:182\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    173\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m    174\u001b[0m         X,\n\u001b[0;32m    175\u001b[0m         y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    179\u001b[0m         accept_large_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    180\u001b[0m     )\n\u001b[1;32m--> 182\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(\n\u001b[0;32m    185\u001b[0m     [] \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m sample_weight, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64\n\u001b[0;32m    186\u001b[0m )\n\u001b[0;32m    187\u001b[0m solver_type \u001b[38;5;241m=\u001b[39m LIBSVM_IMPL\u001b[38;5;241m.\u001b[39mindex(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impl)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:739\u001b[0m, in \u001b[0;36mBaseSVC._validate_targets\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    737\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weight_ \u001b[38;5;241m=\u001b[39m compute_class_weight(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weight, classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m, y\u001b[38;5;241m=\u001b[39my_)\n\u001b[0;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mcls\u001b[39m) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m--> 739\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    740\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe number of classes has to be greater than one; got \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m class\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    741\u001b[0m         \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m    742\u001b[0m     )\n\u001b[0;32m    744\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\n\u001b[0;32m    746\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(y, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: The number of classes has to be greater than one; got 1 class"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Set environment variable to use GPU with TensorFlow\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"StevenLimcorn/indonesian-roberta-base-emotion-classifier\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Use GPU for TensorFlow\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    try:\n",
    "        # Only allocate 1GB of memory for the GPU\n",
    "        tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], \"GPU\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# Load your external labeled dataset\n",
    "data = snorkel_data\n",
    "# Tokenize your text examples and generate embeddings\n",
    "X = []\n",
    "for example in data:\n",
    "    text = example[0]\n",
    "    inputs = tokenizer(text, return_tensors=\"tf\")\n",
    "    outputs = model(inputs)[0]\n",
    "    embeddings = np.mean(outputs.numpy(), axis=1).squeeze()\n",
    "    X.append(embeddings)\n",
    "\n",
    "y = [example[1] for example in data]\n",
    "\n",
    "# Split your data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train your SVM classifier\n",
    "clf = SVC(kernel=\"linear\", C=1.0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate your classifier\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f517d78c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1, -1, -1, -1, -1],\n",
       "       [-1, -1, -1, -1, -1],\n",
       "       [-1, -1, -1, -1, -1],\n",
       "       ...,\n",
       "       [-1, -1, -1, -1, -1],\n",
       "       [-1, -1, -1, -1, -1],\n",
       "       [-1, -1, -1, -1, -1]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c11bc779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "14/14 [==============================] - 4s 74ms/step - loss: 0.0000e+00 - accuracy: 0.1064 - val_loss: 0.0000e+00 - val_accuracy: 0.0472\n",
      "Epoch 2/10\n",
      "14/14 [==============================] - 1s 43ms/step - loss: 0.0000e+00 - accuracy: 0.1064 - val_loss: 0.0000e+00 - val_accuracy: 0.0472\n",
      "Epoch 3/10\n",
      "14/14 [==============================] - 1s 43ms/step - loss: 0.0000e+00 - accuracy: 0.1064 - val_loss: 0.0000e+00 - val_accuracy: 0.0472\n",
      "Epoch 4/10\n",
      "14/14 [==============================] - 1s 43ms/step - loss: 0.0000e+00 - accuracy: 0.1064 - val_loss: 0.0000e+00 - val_accuracy: 0.0472\n",
      "Epoch 5/10\n",
      "14/14 [==============================] - 1s 43ms/step - loss: 0.0000e+00 - accuracy: 0.1064 - val_loss: 0.0000e+00 - val_accuracy: 0.0472\n",
      "Epoch 6/10\n",
      "14/14 [==============================] - 1s 44ms/step - loss: 0.0000e+00 - accuracy: 0.1064 - val_loss: 0.0000e+00 - val_accuracy: 0.0472\n",
      "Epoch 7/10\n",
      "14/14 [==============================] - 1s 43ms/step - loss: 0.0000e+00 - accuracy: 0.1064 - val_loss: 0.0000e+00 - val_accuracy: 0.0472\n",
      "Epoch 8/10\n",
      "14/14 [==============================] - 1s 45ms/step - loss: 0.0000e+00 - accuracy: 0.1064 - val_loss: 0.0000e+00 - val_accuracy: 0.0472\n",
      "Epoch 9/10\n",
      "14/14 [==============================] - 1s 47ms/step - loss: 0.0000e+00 - accuracy: 0.1064 - val_loss: 0.0000e+00 - val_accuracy: 0.0472\n",
      "Epoch 10/10\n",
      "14/14 [==============================] - 1s 47ms/step - loss: 0.0000e+00 - accuracy: 0.1064 - val_loss: 0.0000e+00 - val_accuracy: 0.0472\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - accuracy: 0.0472\n",
      "Test loss: 0.0\n",
      "Test accuracy: 0.04716981202363968\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# Load data from CSV file\n",
    "df = pd.read_csv(\"D:\\Downloads\\Snorkel_dataset_v4.csv\")\n",
    "\n",
    "# Define text and label columns\n",
    "text_col = 'tweet'\n",
    "label_col = 'label'\n",
    "\n",
    "# Convert text labels to numerical labels\n",
    "label_map = {label: i for i, label in enumerate(df[label_col].unique())}\n",
    "df[label_col] = df[label_col].map(label_map)\n",
    "\n",
    "# Tokenize text data\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(df[text_col])\n",
    "X = tokenizer.texts_to_sequences(df[text_col])\n",
    "X = pad_sequences(X, maxlen=50)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_size = int(0.8 * len(df))\n",
    "X_train, y_train = X[:train_size], df[label_col][:train_size]\n",
    "X_test, y_test = X[train_size:], df[label_col][train_size:]\n",
    "\n",
    "# Define LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(5000, 64))\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "329ef2c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>senang</th>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>takut</th>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cinta</th>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sedih</th>\n",
       "      <td>3</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marah</th>\n",
       "      <td>4</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        j Polarity  Coverage  Overlaps  Conflicts\n",
       "senang  0       []       0.0       0.0        0.0\n",
       "takut   1       []       0.0       0.0        0.0\n",
       "cinta   2       []       0.0       0.0        0.0\n",
       "sedih   3       []       0.0       0.0        0.0\n",
       "marah   4       []       0.0       0.0        0.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snorkel.labeling import LFAnalysis\n",
    "\n",
    "LFAnalysis(L=L_train, lfs=lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76418de8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [9], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m records \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m record \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Making the prediction\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m classifier(\n\u001b[1;32m----> 7\u001b[0m         \u001b[43mrecord\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[0;32m      8\u001b[0m     )\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Creating the prediction entity as a list of tuples (label, probability)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     12\u001b[0m         (translate_labels[prediction[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]], prediction[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m prediction \u001b[38;5;129;01min\u001b[39;00m predictions[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     14\u001b[0m     ]\n",
      "\u001b[1;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "records = []\n",
    "\n",
    "for record in data:\n",
    "\n",
    "    # Making the prediction\n",
    "    predictions = classifier(\n",
    "        record[\"content\"],\n",
    "    )\n",
    "\n",
    "    # Creating the prediction entity as a list of tuples (label, probability)\n",
    "    prediction = [\n",
    "        (translate_labels[prediction[\"label\"]], prediction[\"score\"])\n",
    "        for prediction in predictions[0]\n",
    "    ]\n",
    "\n",
    "    # Appending to the record list\n",
    "    records.append(\n",
    "        rb.TextClassificationRecord(\n",
    "            text=record[\"content\"],\n",
    "            prediction=prediction,\n",
    "            prediction_agent=\"https://https://huggingface.co/StevenLimcorn/indonesian-roberta-base-emotion-classifier\",\n",
    "            metadata={\"split\": \"train\"},\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99aeab0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8043377",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
